# Introduction {#intro}
I have started writing this book in 2020 during the COVID-19 pandemic, having figured out that it has been more than 10 years since the publishing of the fundamental textbook of [@Hyndman2008b], who discuss ETS (Error-Trend-Seasonality) framework in the Single Source of Error (SSOE) form. If you are interested in knowing more about exponential smoothing, then this is a must read material on the topic. However, there has been some progress in the area since 2008, and I have developed some models and functions based on ETS, making the framework a bit more flexible and general. Given that the publication of all the aspects of these models in peer-reviewed journals is difficult and challenging, I have decided to summarise all the progress in the book, showing what happens inside the models and how to use the functions in different cases.

Before we move to nitty gritty details of the models, it is important to agree what we are talking about. So, here is a couple of definitions:

- **Statistical model** (or 'stochastic model', or just 'model' in this textbook) is a 'mathematical representation of a real phenomenon with a complete specification of distribution and parameters' [@Svetunkov2017a]. Very roughly, the statistical model is something that contains a structure (defined by its parameters) and a noise that follows some distribution.
- **True model** is the idealistic statistical model that is correctly specified (has all the necessary components in correct form), applied to the data in population. By this definition, true model is never reachable in reality, but it is achievable in theory if for some reason we know what components and variables should definitely be in the model and have all the data in the world.
- **Estimated model** (aka 'used model' or 'applied model') is the statistical model that was constructed and estimated on the available sample of data. This typically differs from the true model, because the latter is not known. Even if the specification of the true model is known for some reason, the parameters of the estimated model will differ from the true parameters due to sampling randomness.
- **Data generating process** (DGP) is an artificial statistical model, showing how the data could be generated in theory. This notion is utopic and can be used in simulation experiments in order to check, how the selected model with the specific estimator behave in a specific setting. In real life, the data is not generated from any process, but is usually based on complex interactions between different agents in a dynamic environment. Note that I make a distinction between DGP and true model, because I do not think that the idea of something being generated using a mathematical formula is helpful. Many statisticians will not agree with me on this distinction.
- **Forecasting method** is a mathematical procedure that generates point and / or interval forecasts, with or without a statistical model [@Svetunkov2017a]. Very roughly, forecasting method is just a way of producing forecasts that does not explain how the components of time series interact with each other. It might be needed in order to filter out the noise and extrapolate the structure.

Later in this book, we will see several examples of statistical models, forecasting methods, DGPs and other notions.

Note that this textbook assumes that the reader is familiar with introductory statistics and knows basic forecasting principles. [@Hyndman2018] can be a good start if you do not know either. We will also use elements of linear algebra to explain some modelling parts, but this will not be the main focus of the textbook and you will be able to skip the more challenging parts without jeopardising the main understanding of the topic.


## Forecasting process and forecasts evaluation {#forecastingProcess}
Before we move to the discussion of models and their estimation it makes sense to discuss the forecasting process and how the forecasts should be evaluated. We should start by saying that forecasting needs to be done for a specific purpose. It should not be done just because you can do that. Forecasts should be useful for specific decisions. And these decisions dictate what forecasts and on what horizons to do.

```{example}
Retailers typically need to order some amount of milk that they will sell over the next week. They do not know, how much they will sell, so they usually order, hoping to satisfy, let us say, 95% of demand. This situation tells us that the forecasts need to be done for a week ahead, they should be cumulative (considering the overal demand during a week before the next order) and that they should focus on an upper bound of a 95% prediction interval. Producing just point forecasts might not be useful in this situation.
```

When you understand how your system works and what sort of forecasts you should produce, then you can start an evaluation process, measuring the performance of several forecasting models / methods and selecting the most appropriate for your data. There are different ways how the performance of models / methods can be measured and compared.

### Measuring accuracy of point forecasts {#errorMeasures}
We start with a situation, when point forecasts are of the main interest. In this case we typically start by splitting the available data into train and test sets, and apply the models under consideration to the first one, producing the forecasts for the second, not showing that part to the model. This is called "fixed origin" approach: we fix the point in time, from which to produce forecasts, we produce them, calculate some sort of error measures and compare the models.

There are different error measures that can be used in this case, the selection of one depends on the specific needs. Here we briefly discuss them, noting that the topic has already been extensively discussed in different sources [@Davydenko2013; @SvetunkovAccuracy2019; @SvetunkovAPEs2019]. Here we discuss only the main aspects of the error measures.

The very basic error measures are Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE):
\begin{equation}
    \text{RMSE} = \sqrt{\frac{1}{h} \sum_{j=1}^h \left( y_{t+j} - \hat{y}_{t+j} \right)^2 },
    (\#eq:RMSE)
\end{equation}

\begin{equation}
    \text{MAE} = \frac{1}{h} \sum_{j=1}^h \left| y_{t+j} - \hat{y}_{t+j} \right| ,
    (\#eq:MAE)
\end{equation}
where $y_{t+j}$ is the actual value $j$ steps ahead from the holdout, $\hat{y}_{t+j}$ is the $j$ steps ahead point forecast (conditional expectation of the model) and $h$ is the forecast horizon. As you see, these error measures aggregate the performance of competing forecasting methods across the forecasting horizon, averaging out the specific performances on each $j$. If this information needs to be retained, then the summation can be dropped to obtain just "SE" and "AE".

It is well-known [see, for example, @Kolassa2016] that RMSE is minimised by the mean value of a distribution, and MAE is minimised by the median. So, when selecting between the two, you should consider this property.

The main advantage of these error measures is that they are very simple and have a clear interprertation: they show the average distance from the point forecasts to the actual values. They are perfect if you work with only one time series. However, they are not suitable, when you have several time series and want to see the performance of methods across them. This is mainly because they are scale dependent and contain specific units: if you measures sales of bananas in pounds, then MAE and RMSE will show the error in pounds. And, as we know, you should not add up pounds of bananas with pounds of apples - the result might not make sense.

In order to tackle this issue, different error scaling techniques have been proposed over the years, resulting in a zoo of error measures:

1. MAPE - Mean Absolute Percentage Error;
2. MASE - Mean Absolute Scaled Error [@Hyndman2006];
3. rMAE - Relative Mean Absolute Error [@Davydenko2013];
4. sMAE - scaled Mean Absolute Error [@Petropoulos2015];
5. and others.

They have their own advantages and disadvantages, and we will not discuss them here. It should suffice to say that the selection of error measure should be dictated by the needs of the forecaster. If you want a robust measure that works consistently, but do not care about the interpretation, then go with MASE. If you want an interpretation, then either go with rMAE, or sMAE. And you typically should avoid MAPE and other Percentage Error measures, because they are highly influenced by the actual values you have in the holdout. Furthermore, similarly to the measures above, one can propose RMSE-based scaled and relative error measures.

Finally, when aggregating performance of forecasting methods across several time series, sometimes it makes sense to look at the distribution of errors - this way you will know, which of the methods fails seriously, and which does a consistently good job.

### Measuring uncertainty

### Rolling origin

