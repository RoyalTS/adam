# Forecasting process and forecasts evaluation {#forecastingProcess}
One more thing to discuss before moving to the meat of the textbook, is how to evaluate forecasting models and methods. We should start by saying that forecasting needs to be done for a specific purpose. It should not be done just because you can do that. Forecasts should be useful for specific decisions. And these decisions dictate what forecasts, in what form and on what horizons are needed.

```{example}
Retailers typically need to order some amount of milk that they will sell over the next week. They do not know, how much they will sell, so they usually order, hoping to satisfy, let us say, 95% of demand. This situation tells us that the forecasts need to be done for a week ahead, they should be cumulative (considering the overal demand during a week before the next order) and that they should focus on an upper bound of a 95% prediction interval. Producing just point forecasts might not be useful in this situation.
```

When you understand how your system works and what sort of forecasts you should produce, then you can start an evaluation process, measuring the performance of several forecasting models / methods and selecting the most appropriate for your data. There are different ways how the performance of models / methods can be measured and compared. In this chapte, we discuss the most common approaches.


## Measuring accuracy of point forecasts {#errorMeasures}
We start with a situation, when point forecasts are of the main interest. In this case we typically start by splitting the available data into train and test sets, and apply the models under consideration to the first one, producing the forecasts for the second, not showing that part to the model. This is called "fixed origin" approach: we fix the point in time, from which to produce forecasts, we produce them, calculate some sort of error measures and compare the models.

There are different error measures that can be used in this case, the selection of one depends on the specific needs. Here we briefly discuss them, noting that the topic has already been extensively discussed in different sources [@Davydenko2013; @SvetunkovAccuracy2019; @SvetunkovAPEs2017]. Here we discuss only the main aspects of the error measures.

The very basic error measures are Root Mean Squared Error (RMSE):
\begin{equation}
    \mathrm{RMSE} = \sqrt{\frac{1}{h} \sum_{j=1}^h \left( y_{t+j} - \hat{y}_{t+j} \right)^2 },
    (\#eq:RMSE)
\end{equation}
and Mean Absolute Error (MAE):
\begin{equation}
    \mathrm{MAE} = \frac{1}{h} \sum_{j=1}^h \left| y_{t+j} - \hat{y}_{t+j} \right| ,
    (\#eq:MAE)
\end{equation}
where $y_{t+j}$ is the actual value $j$ steps ahead from the holdout, $\hat{y}_{t+j}$ is the $j$ steps ahead point forecast (conditional expectation of the model) and $h$ is the forecast horizon. As you see, these error measures aggregate the performance of competing forecasting methods across the forecasting horizon, averaging out the specific performances on each $j$. If this information needs to be retained, then the summation can be dropped to obtain just "SE" and "AE".

It is well-known [see, for example, @Kolassa2016] that **RMSE is minimised by the mean value** of a distribution, and **MAE is minimised by the median**. So, when selecting between the two, you should consider this property. This mean, for example, that MAE-based error measures should not be used for the evaluation of models on intermittent demand.

The main advantage of these error measures is that they are very simple and have a clear interprertation: they show the average distance from the point forecasts to the actual values. They are perfect if you work with only one time series. However, they are not suitable, when you have several time series and want to see the performance of methods across them. This is mainly because they are scale dependent and contain specific units: if you measures sales of bananas in pounds, then MAE and RMSE will show the error in pounds. And, as we know, you should not add up pounds of bananas with pounds of apples - the result might not make sense.

In order to tackle this issue, different error scaling techniques have been proposed over the years, resulting in a zoo of error measures:

1. MAPE - Mean Absolute Percentage Error:
\begin{equation}
    \mathrm{MAPE} = \frac{1}{h} \sum_{j=1}^h \frac{y_{t+j} - \hat{y}_{t+j}}{y_{t+j}},
    (\#eq:MAPE)
\end{equation}
2. MASE - Mean Absolute Scaled Error [@Hyndman2006]:
\begin{equation}
    \mathrm{MASE} = \frac{1}{h} \sum_{j=1}^h \frac{|y_{t+j} - \hat{y}_{t+j}|}{\bar{\Delta}_y},
    (\#eq:MASE)
\end{equation}
where $\bar{\Delta}_y = \frac{1}{t-1}\sum_{j=2}^t |\Delta_{y,j}|$ is the mean absolute value of the first differences $\Delta_{y,j}=y_j-y_{j-1}$ of the in-sample data;
3. rMAE - Relative Mean Absolute Error [@Davydenko2013]:
\begin{equation}
    \mathrm{rMAE} = \frac{\mathrm{MAE}_a}{\mathrm{MAE}_b},
    (\#eq:rMAE)
\end{equation}
where $\mathrm{MAE}_a$ is the mean absolute error of the model under consideration and $\mathrm{MAE}_b$ is the MAE of the benchmark model;
4. sMAE - scaled Mean Absolute Error [@Petropoulos2015]:
\begin{equation}
    \mathrm{sMAE} = \frac{\mathrm{MAE}}{\bar{y}},
    (\#eq:sMAE)
\end{equation}
where $\bar{y}$ is the mean of the in-sample data.
5. and others.

There is no "the best" error measure, all of them have their advantages and disadvantages. For example:

1. MAPE is scale sensitive (if the actual values are measured in thousands of units, the resulting error will be much lower than in the case of hundreds of units) and cannot be estimated on data with zeroes. However, it has a simple interpretation as it shows the percentage error (as the name suggests);
2. MASE does not have issues of MAPE, but it also does not have a simple interpretation due to the division by the first differences of the data (some interpret this as an in-sample one step ahead naive forecast);
3. rMAE does not have issues of MAPE, has a simple interpretation (it shows by how much one model is better than the other), but fails, when either $\mathrm{MAE}_a$ or $\mathrm{MAE}_b$ for a specific time series is equal to zero;
4. sMAE does not have issues of MAPE, but has an interpretation close to it, however it breaks down, when the data exhibits trends.

As a result, when comparing different forecasting methods, it makes sense calculating several of the error measures for the purposes of the comparison. Also note that the choice of the metric might depend on the specific needs in the company or the forecaster. If you want a robust measure that works consistently, but you do not care about the interpretation, then go with MASE. If you want an interpretation, then either go with rMAE, or sMAE. And you typically should avoid MAPE and other Percentage Error measures, because they are highly influenced by the actual values you have in the holdout. Furthermore, similarly to the measures above, one can propose RMSE-based scaled and relative error measures, which would measure the performance of methods in terms of means rather than medians.

Finally, when aggregating performance of forecasting methods across several time series, sometimes it makes sense to look at the distribution of errors - this way you will know, which of the methods fails seriously, and which does a consistently good job.


## Measuring uncertainty
While point forecasts are useful, prediction intervals are important in many areas of application as well. They allow getting an understanding about the uncertainty around the point forecasts and thus allow making less risky decisions. 


## Rolling origin {#rollingOrigin}
```{block, type="remark"}
The text in this section is based on the vignette for the [greybox package](https://cran.r-project.org/package=greybox), written by the author of this textbook.
```
When there is a need to select the most appropriate forecasting model or method for the data, the forecasters usually split the available sample into two parts: in-sample (aka "training set") and holdout sample (or out-sample, or "test set"). The model is then estimated on in-sample and its forecasting performance is evaluated [using some error measure](#errorMeasures) on the holdout sample.

If such a procedure done only once, then this is called "fixed origin" evaluation. However, the time series might contain outliers or level shifts and a poor model might perform better than the more appropriate one only because of that. In order to robustify the evaluation of models, something called "rolling origin" is used.

Rolling origin is an evaluation technique according to which the forecasting origin is updated successively and the forecasts are produced from each origin [@Tashman2000]. This technique allows obtaining several forecast errors for time series, which gives a better understanding of how the models perform. This can be considered as a time series analogue for cross-validation techniques [@WikipediaCrossValidation2020]. There are different options of how this can be done.

### Principles of Rolling origin

The figure \@ref(fig:ROProcessCO) [@Svetunkov2017] depicts the basic idea of rolling origin. White cells correspond to the in-sample data, while the light grey cells correspond to the three-steps-ahead forecasts. Time series has 25 observations in that figure, and the forecasts are produced from 8 origins, starting from the origin 15. The model is re-estimated on each iteration, and the forecasts are produced. After that a new observation is added at the end of the series and the procedure continues. The process stops when there is no more data to add. This could be considered as a rolling origin with a **constant holdout** sample size. As a result of this procedure 8 one to three steps ahead forecasts are produced. Based on them we can calculate the preferred error measures and choose the best performing model.

```{r ROProcessCO, out.width="75%", echo=FALSE, fig.cap="Rolling Origin with constant holdout size"}
# All defaults
knitr::include_graphics("./images/03-ROProcessCO.gif")
```

Another option of producing forecasts from 8 origins would be to start from the origin 17 instead of 15, as shown on Figure \@ref(fig:ROProcessNoCO). In this case the procedure continues until origin 22, when the last three-steps-ahead forecast is produced, and then continues with the decreasing forecasting horizon. So the two-steps-ahead forecast is produced from the origin 23 and only one-step-ahead forecast is produced from the origin 24. As a result we obtain 8 one-step-ahead forecasts, 7 two-steps-ahead forecasts and 6 three-steps-ahead forecasts. This can be considered as a rolling origin with a **non-constant holdout** sample size. This can be useful in cases of small samples, when we don't have any observations to spare.

```{r ROProcessNoCO, out.width="75%", echo=FALSE, fig.cap="Rolling Origin with non-constant holdout size"}
# All defaults
knitr::include_graphics("./images/03-ROProcessNoCO.gif")
```

Finally, in both of the cases above we had the **increasing in-sample** size. However for some research purposes we might need a **constant in-sample**. The figure \@ref(fig:ROProcessCOCI) demonstrates such situation. In this case, on each iteration we add an observation at the end of the series and remove one from the beginning of the series (dark grey cells).

```{r ROProcessCOCI, out.width="75%", echo=FALSE, fig.cap="Rolling Origin with constant in-sample size"}
# All defaults
knitr::include_graphics("./images/03-ROProcessCOCI.gif")
```

### Rolling origin in R
The function `ro()` from `greybox` package (written by Yves Sagaert and Ivan Svetunkov in 2016 on the way to the International Symposium on Forecasting) implements the rolling origin evaluation for any function you like with a predefined `call` and returns the desired `value`. It heavily relies on the two variables: `call` and `value` - so it is quite important to understand how to formulate them in order to get the desired results. Overall, `ro()` is a very flexible function, but, as a result, it is not very simple. In this subsection we will see how it work on a couple of examples.

We start with a simple example, generating series from normal distribution:
```{r}
x <- rnorm(100,100,10)
```

We use ARIMA(0,1,1) model implemented in `stats` package:
```{r}
ourCall <- "predict(arima(x=data,order=c(0,1,1)),n.ahead=h)"
```

The call that we specify includes two important elements: `data` and `h`. `data` specifies where the in-sample values are located in the function that we want to use, and **it needs to be called "data"** in the call. `h` will tell our function, where the forecasting horizon is specified in the selected function. Note that in this example we use `arima(x=data,order=c(0,1,1))`, which produces a desired ARIMA(0,1,1) model and then we use `predict(..., n.ahead=h)`, which produces an h steps ahead forecast from that model.

Having the call, we need also to specify what the function should return. This can be the conditional mean (point forecasts), prediction intervals, the parameters of a model, or, in fact, anything that the model returns (e.g. name of the fitted model and its likelihood). However, there are some differences in what the `ro()` returns depending on what the function returns. If it is a vector, then `ro()` will produce a matrix (with values for each origin in columns). If it is a matrix, then an array is returned. Finally, if it is a list, then a list of lists is returned.

In order not to overcomplicate things, we will collect the conditional mean from the `predict()` function:
```{r}
ourValue <- c("pred")
```

**NOTE**: If you do not specify the value to return, the function will try to return everything, but it might fail, especially if a lot of values are returned. So, in order to be on the safe side, **always provide the `value`, when possible**.

Now that we have specified `ourCall` and `ourValue`, we can produce forecasts from the model using rolling origin. Let's say that we want three-steps-ahead forecasts and 8 origins with the default values of all the other parameters:
```{r}
returnedValues1 <- ro(x, h=3, origins=8, call=ourCall, value=ourValue)
```

The function returns a list with all the values that we asked for plus the actual values from the holdout sample. We can calculate some basic error measure based on those values, for example, scaled Mean Absolute Error [@Petropoulos2015]:
```{r}
apply(abs(returnedValues1$holdout - returnedValues1$pred),1,mean,na.rm=TRUE) / mean(returnedValues1$actuals)
```

In this example we use `apply()` function in order to distinguish between the different forecasting horizons and have an idea of how the model performs for each of them. These numbers do not tell us much on their own, but if we compared the performance of this model with another one, then we could infer if one model is more appropriate for the data than the other one. For example, applying ARIMA(1,1,2) to the same data, we will get:
```{r}
ourCall <- "predict(arima(x=data,order=c(1,1,2)),n.ahead=h)"
returnedValues2 <- ro(x, h=3, origins=8, call=ourCall, value=ourValue)
apply(abs(returnedValues2$holdout - returnedValues2$pred),1,mean,na.rm=TRUE) / mean(returnedValues2$actuals)
```
Comparing these errors with the ones from the previous model, we can conclude, which of the approaches is more adequate for the data.

We can also plot the forecasts from the rolling origin, which shows how the selected model behaves:
```{r fig.width=8, fig.height=6}
par(mfcol=c(2,1))
plot(returnedValues1)
plot(returnedValues2)
```

In this example the forecasts from different origins are close to each other. This is because the data is stationary and the model is quite stable.

The rolling origin function from `greybox` package also allows working with explanatory variables and returning prediction intervals, if needed. Some further examples are discussed in the vignette of the package: `vignette("ro","greybox")`.


## Statistical tests


