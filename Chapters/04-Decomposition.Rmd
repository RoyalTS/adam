# Time series decomposition and ETS taxonomy {#tsDecomposition}

A very important topic that we need to discuss before we move to the state space models, ETS, ARIMA and other things, is the time series decomposition because it lies in the core of ETS models.

The main idea behind the decomposition is that any time series can contain several unobservable components, such as:

1. Level of the series,
2. Growth of the series,
3. Seasonality,
4. Error.

Depending on a textbook or on a paper you are dealing with, you might have different names for these components. For example, in classical decomposition [@Persons1919] it is assumed that (1) and (2) represent a specific component called "trend", so the typical model contains error, trend and seasonality. There are modifications of this, which also contain cyclical component. When it comes to ETS, the growth component (2) is called "trend", so the model consists of the four components. We will use the ETS notations in this textbook. According to it, the components can interact with each other differently: either via addition or multiplication. The pure additive model in this case can be summarised as:
\begin{equation}
    y_t = l_{t-1} + b_{t-1} + s_{t-m} + \epsilon_t ,
    (\#eq:PureAdditive)
\end{equation}
where $l_{t-1}$ is the level, $b_{t-1}$ is the trend, $s_{t-m}$ is the seasonal component with periodicity $m$ (e.g. 12 for monthly, implying that something is repeated every 12 months) - all these components are produced on the previous observations and are used on the current one. Finally, $\epsilon_t$ is the error term, which follows some distribution and has zero mean. Similarly, the pure multiplicative model is:
\begin{equation}
    y_t = l_{t-1} b_{t-1} s_{t-m} \varepsilon_t ,
    (\#eq:PureMultiplicative)
\end{equation}
where $\varepsilon_t$ is the error term that has mean of one. The interpretation of the model \@ref(eq:PureAdditive) is that the different components add up to each other, so, for example, the sales in January typically increase by the amount $s_{t-m}$, and that there is still some randomness that is not taken into account in the model. The pure additive models can be applied for the data that can have positive, negative and zero values. In case of the model \@ref(eq:PureMultiplicative), the interpretation is similar, but the sales change by $(s_{t-m}-1) \text{%}$ from the baseline. These models only work with the data with positive values. Although they should also work on data with purely negative values as well, this is less often met in practice.

There are many different techniques on how to extract the components from time series, the most popular of which is classical decomposition, which aims at smoothing each unobserved components in the data. An example of this is the `decompose()` function from `stats` package. Here is an example with pure multiplicative model and `AirPassengers` data in R:
```{r decomposeAirPassengers}
ourDecomposition <- decompose(AirPassengers, type="multiplicative")
plot(ourDecomposition)
```

Note that the trend component in `decompose()` function is in fact $l_{t-1}+b_{t-1}$. If the decomposition is done correctly, then different components can then be forecasted and mixed in order to obtain the point forecasts.

It is also possible to define mixed models, for example, when trend is additive, but the other components are multiplicative:
\begin{equation}
    y_t = (l_{t-1} + b_{t-1}) s_{t-m} \varepsilon_t ,
    (\#eq:MixedAdditiveTrend)
\end{equation}
these models work well in practice, when the data has high values, far from zero. In the other cases, they might produce contradicting results: e.g., generate negative values on positive data.

Based on the type of error, trend and seasonality, [@Pegels1969] proposed a taxonomy, which was then developed further by [@Hyndman2002] and refined by [@Hyndman2008b]. According to this taxonomy, error, trend and seasonality can be:

1. Error: either "Additive", or "Multiplicative";
2. Trend: either "None", or "Additive", or "Additive damped", or "Multiplicative", or "Multiplicative damped";
3. Seasonality: either "None", or "Additive", or "Multiplicative".

So, the ETS stands for "Error-Trend-Seasonality" and defines how specifically the components interact with each other. According to this taxonomy, the model \@ref(eq:PureAdditive) is ETS(A,A,A), while the model \@ref(eq:PureMultiplicative) is ETS(M,M,M), and \@ref(eq:MixedAdditiveTrend) is ETS(M,A,M).

The main advantage of ETS taxonomy is that the components have clear interpretation, and that it is flexible, allowing to have 30 models with different types of error, trend and seasonality. The figure below shows examples of different time series with deterministic (they do not change over time) level, trend and seasonality, based on how they interact in the model. The first one shows the additive error case:
```{r echo=FALSE}
modelsList <- c("ANN","AAN","AAdN","AMN","AMdN","ANA","AAA","AAdA","AMA","AMdA","ANM","AAM","AAdM","AMM","AMdM",
                "MNN","MAN","MAdN","MMN","MMdN","MNA","MAA","MAdA","MMA","MMdA","MNM","MAM","MAdM","MMM","MMdM")
level <- 500
trend <- c(100,1.05)
seasonality <- list((c(1.3,1.1,0.9,0.75)-1)*2*level,c(1.3,1.1,0.9,0.75))
generatedData <- vector("list", length(modelsList))
scale <- 0.05
for(i in 1:length(modelsList)){
  initial <- switch(substr(modelsList[i],2,2),
                    "A"=c(level,trend[1]),
                    "M"=c(level,trend[2]),
                    level*2);
  initialSeason <- switch(substr(modelsList[i],nchar(modelsList[i]),nchar(modelsList[i])),
                    "A"=seasonality[[1]],
                    "M"=seasonality[[2]],
                    NULL);
  if(nchar(modelsList[i])==4){
    phi <- 0.95;
  }
  else{
    phi <- 1;
  }
  sdValue <- switch(substr(modelsList[i],1,1),
                    "A"=sqrt(level^2*(exp(scale^2)-1)*exp(scale)),
                    "M"=scale)
  meanValue <- switch(substr(modelsList[i],1,1),
                      "A"=0,
                      "M"=1)
  generatedData[[i]] <- sim.es(modelsList[i], obs=36, frequency=4, persistence=0, phi=phi,
                               initial=initial, initialSeason=initialSeason, mean=meanValue, sd=sdValue)$data
}

# Prepare the canvas
par(mfcol=c(5,3),mar=c(2,3,2,1))
# The matrix that corresponds to the i
ylimValues <- matrix(c(1:15),5,3)
for(i in 1:15){
  if(i>15){
    ylimRow <- which(ylimValues == i-15,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]+15]))
  }
  else{
    ylimRow <- which(ylimValues == i,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]]))
  }
  plot(generatedData[[i]], main=paste0("ETS(",modelsList[i],")"),ylim=ylim,ylab="")
}
```

The things to note:

1. When the seasonality is multiplicative, then its amplitude increases with the increase of the level of the data. The amplitude of seasonality does not change for the additive seasonal models. e.g. compare the ETS(A,A,A) with ETS(A,A,M): the distance between the highest and the lowest points for the form  in the first year is roughly the same as in the last year. In case of ETS(A,A,M), the distance increases with the increase of level;
2. When the trend is multiplicative, it corresponds to the exponential growh / decay. So, in this situation we say that there is a roughly 5% growth in the data;
3. The damped trend models slow down both additive and multiplicative trends.

And here are the similar plots for the multiplicative error models:
```{r echo=FALSE}
# Prepare the canvas
par(mfcol=c(5,3),mar=c(2,3,2,1))
# The matrix that corresponds to the i
ylimValues <- matrix(c(1:15),5,3)
for(i in 16:30){
  if(i>15){
    ylimRow <- which(ylimValues == i-15,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]+15]))
  }
  else{
    ylimRow <- which(ylimValues == i,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]]))
  }
  plot(generatedData[[i]], main=paste0("ETS(",modelsList[i],")"),ylim=ylim,ylab="")
}
```

They show roughly the same picture, the main difference being that the variance of the error increases with the increase of the level of the data - this becomes clearer on ETS(M,A,N) and ETS(M,M,N) data. This effect is called heteroscedasticity in statistics, and [@Hyndman2008b] argue that the main benefit of the multiplicative error models is in being able to capture this feature.

In the next several chapters we will discuss the basic ETS models from this taxonomy.
