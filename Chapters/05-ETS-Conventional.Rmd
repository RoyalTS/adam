# Conventional ETS model

In this chapter we discuss briefly the conventional ETS, as formulated in @Hyndman2008b, discuss its assumptions, properties, features, limitations and how the model is typically constructed and estimated. We start with the ETS taxonomy.


## ETS taxonomy {#ETSTaxonomy}
Now building upon the idea of [time series components](#tsComponents), we can move to the ETS taxonomy. Based on the type of error, trend and seasonality, [@Pegels1969] proposed a taxonomy, which was then developed further by [@Hyndman2002] and refined by [@Hyndman2008b]. According to this taxonomy, error, trend and seasonality can be:

1. Error: either "Additive" (A), or "Multiplicative" (M);
2. Trend: either "None" (N), or "Additive" (A), or "Additive damped" (Ad), or "Multiplicative" (M), or "Multiplicative damped" (Md);
3. Seasonality: either "None" (N), or "Additive" (A), or "Multiplicative" (M).

So, the ETS stands for "Error-Trend-Seasonality" and defines how specifically the components interact with each other. According to this taxonomy, the model \@ref(eq:PureAdditive) is ETS(A,A,A), while the model \@ref(eq:PureMultiplicative) is ETS(M,M,M), and \@ref(eq:MixedAdditiveTrend) is ETS(M,A,M).

The main advantage of ETS taxonomy is that the components have clear interpretation, and that it is flexible, allowing to have 30 models with different types of error, trend and seasonality. The figure below shows examples of different time series with deterministic (they do not change over time) level, trend and seasonality, based on how they interact in the model. The first one shows the additive error case:
```{r ETSTaxonomyAdditive, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Time series corresponding to the additive error ETS models"}
modelsList <- c("ANN","AAN","AAdN","AMN","AMdN","ANA","AAA","AAdA","AMA","AMdA","ANM","AAM","AAdM","AMM","AMdM",
                "MNN","MAN","MAdN","MMN","MMdN","MNA","MAA","MAdA","MMA","MMdA","MNM","MAM","MAdM","MMM","MMdM")
level <- 500
trend <- c(100,1.05)
seasonality <- list((c(1.3,1.1,0.9,0.75)-1)*2*level,c(1.3,1.1,0.9,0.75))
generatedData <- vector("list", length(modelsList))
scale <- 0.05
for(i in 1:length(modelsList)){
  initial <- switch(substr(modelsList[i],2,2),
                    "A"=c(level,trend[1]),
                    "M"=c(level,trend[2]),
                    level*2);
  initialSeason <- switch(substr(modelsList[i],nchar(modelsList[i]),nchar(modelsList[i])),
                    "A"=seasonality[[1]],
                    "M"=seasonality[[2]],
                    NULL);
  if(nchar(modelsList[i])==4){
    phi <- 0.95;
  }
  else{
    phi <- 1;
  }
  sdValue <- switch(substr(modelsList[i],1,1),
                    "A"=sqrt(level^2*(exp(scale^2)-1)*exp(scale)),
                    "M"=scale)
  meanValue <- switch(substr(modelsList[i],1,1),
                      "A"=0,
                      "M"=1)
  generatedData[[i]] <- smooth::sim.es(modelsList[i], obs=36, frequency=4, persistence=0, phi=phi,
                                       initial=initial, initialSeason=initialSeason, mean=meanValue, sd=sdValue)$data
}

# Prepare the canvas
par(mfcol=c(5,3),mar=c(2,3,2,1))
# The matrix that corresponds to the i
ylimValues <- matrix(c(1:15),5,3)
for(i in 1:15){
  if(i>15){
    ylimRow <- which(ylimValues == i-15,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]+15]))
  }
  else{
    ylimRow <- which(ylimValues == i,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]]))
  }
  plot(generatedData[[i]], main=paste0("ETS(",modelsList[i],")"),ylim=ylim,ylab="")
}
```

Things to note from this plot:

1. When the seasonality is multiplicative, its amplitude increases with the increase of the level of the data. The amplitude of seasonality does not change for the additive seasonal models. e.g. compare the ETS(A,A,A) with ETS(A,A,M): the distance between the highest and the lowest points for the former in the first year is roughly the same as in the last year. In the case of ETS(A,A,M), the distance increases with the increase of level;
2. When the trend is multiplicative, it corresponds to the exponential growh / decay. So, in this situation with ETS(A,M,N) we say that there is a roughly 5% growth in the data;
3. The damped trend models slow down both additive and multiplicative trends;
4. It is practically impossible to distinguish the additive and multiplicative seasonality if the average of the data is the same - this becomes apparent on the example of ETS(A,N,A) and ETS(A,N,M).

And here is a similar plot for the multiplicative error models:
```{r ETSTaxonomyMultiplicative, echo=FALSE, warning=FALSE, fig.cap="Time series corresponding to the multiplicative error ETS models"}
# Prepare the canvas
par(mfcol=c(5,3),mar=c(2,3,2,1))
# The matrix that corresponds to the i
ylimValues <- matrix(c(1:15),5,3)
for(i in 16:30){
  if(i>15){
    ylimRow <- which(ylimValues == i-15,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]+15]))
  }
  else{
    ylimRow <- which(ylimValues == i,arr.ind=T)[1]
    ylim <- range(unlist(generatedData[ylimValues[ylimRow,]]))
  }
  plot(generatedData[[i]], main=paste0("ETS(",modelsList[i],")"),ylim=ylim,ylab="")
}
```

They show roughly the same picture as the additive case, with the main difference being that the variance of the error increases with the increase of the average value of the data - this becomes clearer on ETS(M,A,N) and ETS(M,M,N) data. This effect is called heteroscedasticity in statistics, and [@Hyndman2008b] argue that the main benefit of the multiplicative error models is in being able to capture this feature.

In the next several chapters we will discuss the basic ETS models from this taxonomy. Note that not all the models in this taxonomy make sense, and some of them are typically dropped from consideration. Although ADAM implements all o fhtme, we will discuss the potential issues with them and what to expect from them.


## Mathematical models in the ETS taxonomy {#ETSTaxonomyMaths}
I hope that it becomes clearer to the reader how the ETS framework is built upon the idea of [time series decomposition](#tsComponents). By introducing different components and defining their types and by adding the equations for their update, we can construct models that would work better on the time series at hands. The equations discussed in [the previous section](#tsComponents) represent so called "measurement" or "observation" equations of the ETS models. But we should also take into account the potential change in components over time. The "transition" or "state" equation is supposed to reflect this change: they explain, how the level, trend or seasonal components change over time.

As discussed in [the previois section](#ETSTaxonomy), given different types of components and their interactions, we end up with 30 models in the taxonomy. Tables \@ref(tab:ETSAdditiveError) and \@ref(tab:ETSMultiplicativeError) summarise mathematically all 30 ETS models shown graphically on Figures \@ref(fig:ETSTaxonomyAdditive) and \@ref(fig:ETSTaxonomyMultiplicative) in the [ETS Taxonomy chapter](#ETSTaxonomy), presenting formulae for:

- Measurement equation;
- Transition equation;
- Conditional one step ahead expectation $\mu_{y,t|t-1}$;
- Multiple steps ahead point forecast $\hat{y}_{t+h}$;
- Conditional multiple steps ahead expectation $\mu_{y,t+h|t}$;

In case of the additive error models, the point forecasts correspond to the expectations only when the expectation of the error term is zero, $\text{E}(\epsilon_t)=0$, while in case of the multiplicative one the condition is typically that $\text{E}(1+\epsilon_t)=1$. 
```{block, type="remark"}
However, **note that not all the point forecasts correspond to the conditional expectations**. This issue applies to the models with multiplicative trend and / or multiplicative seasonality. This is because SSOE models assume that different states are correlated (they have the same source of error) and as a result multiple steps ahead values (when h>1) of states introduce products of error terms. So, the conditional expectations in these cases might not have an analytical forms, and when working with these models, simulations might be required. This does not apply to the one step ahead forecasts, for which the classical formulae work.
```

```{r ETSAdditiveError, echo=FALSE}
# T="N"
etsAdditiveTable <- c("$\\begin{split}
      &y_{t} = l_{t-1} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\alpha \\epsilon_t \\\\
      &\\mu_{y,t|t-1} = l_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\alpha \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t \\\\
      &\\mu_{y,t|t-1} = l_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1}} \\\\
      &\\mu_{y,t|t-1} = l_{t-1} s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{split}$",
# T="A"
    "$\\begin{split}
      &y_{t} = l_{t-1} + b_{t-1} + \\epsilon_t \\\\
      &l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\epsilon_t \\\\
      &\\mu_{y,t|t-1} = l_{t-1} + b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} + h b_t \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} + b_{t-1} + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t \\\\
      &\\mu_{y,t|t-1} = l_{t-1} + b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + h b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} + b_{t-1}) s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} + b_{t-1}} \\\\
      &\\mu_{y,t|t-1} = (l_{t-1} + b_{t-1}) s_{t-m} \\\\
      &\\hat{y}_{t+h} = \\left(l_{t} + h b_{t-1}\\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{split}$",
# T="Ad"
    "$\\begin{split}
      &y_{t} = l_{t-1} + \\phi b_{t-1} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = \\phi b_{t-1} + \\beta \\epsilon_t \\\\
      &\\mu_{y,t|t-1} = l_{t-1} + \\phi b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_t \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} + \\phi b_{t-1} + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = \\phi b_{t-1} + \\beta \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t \\\\
      &\\mu_{y,t|t-1} = l_{t-1} + \\phi b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &b_t = \\phi b_{t-1} + \\beta \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} + \\phi b_{t-1}} \\\\
      &\\mu_{y,t|t-1} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} \\\\
      &\\hat{y}_{t+h} = \\left(l_{t} + \\sum_{j=1}^h \\phi^j b_t \\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{split}$",
# T="M"
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\
      &\\mu_{y,t|t-1} = l_{t-1} b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} b_t^h \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1} + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1} + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t \\\\
      &\\mu_{y,t|t-1} = l_{t-1} b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^h + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1} s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1} + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}} \\\\
      &\\mu_{y,t|t-1} = l_{t-1} b_{t-1} s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^h s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$",
# T="Md"
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1}^\\phi + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\
      &\\mu_{y,t|t-1} = l_{t-1} b_{t-1}^\\phi \\\\
      &\\hat{y}_{t+h} = l_{t} b_t^{\\sum_{j=1}^h \\phi^j} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1}^\\phi + s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\epsilon_t \\\\
      &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}} \\\\
      &s_t = s_{t-m} + \\gamma \\epsilon_t \\\\
      &\\mu_{y,t|t-1} = l_{t-1} b_{t-1}^\\phi + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1}^\\phi s_{t-m} + \\epsilon_t \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\frac{\\epsilon_t}{s_{t-m}} \\\\
      &b_t = b_{t-1}^\\phi + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}} \\\\
      &s_t = s_{t-m} + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}} \\\\
      &\\mu_{y,t|t-1} = l_{t-1} b_{t-1}^\\phi s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$")
etsAdditiveTable <- matrix(etsAdditiveTable, 5, 3, byrow=TRUE,
                           dimnames=list(c("**No trend**","**Additive trend**","**Additive damped trend**",
                                           "**Multiplicative trend**","**Multiplicative damped trend**"),
                                         c("N","A","M")))
kableTable <- kableExtra::kable(etsAdditiveTable, escape=FALSE, caption="Additive error ETS models",
                                col.names=c("Nonseasonal","Additive seasonality","Multiplicative seasonality"))
kable_styling(kableTable, font_size=12, protect_latex=TRUE)
```

The multiplicative error models have the same one step ahead expectations as the additive error ones, but due to the multiplication by the error term, the multiple steps ahead conditional expectations between the two models might differ, specifically for the multiplicative trend and multiplicative seasonal models.

```{r ETSMultiplicativeError, echo=FALSE}
# T="N"
etsMultiplicativeTable <- c("$\\begin{split}
      &y_{t} = l_{t-1}(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\\\
      &\\mu_{y,t|t-1} = l_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} + \\alpha \\mu_{y,t|t-1} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t|t-1} \\epsilon_t \\\\
      &\\mu_{y,t|t-1} = l_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} s_{t-m}(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1}(1 + \\alpha \\epsilon_t) \\\\
      &s_t = s_{t-m}(1 + \\gamma \\epsilon_t) \\\\
      &\\mu_{y,t|t-1} = l_{t-1} s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{split}$",
# T="A"
    "$\\begin{split}
      &y_{t} = (l_{t-1} + b_{t-1})(1 + \\epsilon_t) \\\\
      &l_t = (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1} + \\beta \\mu_{y,t|t-1} \\epsilon_t \\\\
      &\\mu_{y,t|t-1} = l_{t-1} + b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} + h b_t \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} + b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} + b_{t-1} + \\alpha \\mu_{y,t|t-1} \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\mu_{y,t|t-1} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t|t-1} \\epsilon_t \\\\
      &\\mu_{y,t|t-1} = l_{t-1} + b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + h b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} + b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\
      &l_t = (l_{t-1} + b_{t-1})(1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1} + \\beta (l_{t-1} + b_{t-1}) \\epsilon_t \\\\
      &s_t = s_{t-m} (1 + \\gamma \\epsilon_t) \\\\
      &\\mu_{y,t|t-1} = (l_{t-1} + b_{t-1}) s_{t-m} \\\\
      &\\hat{y}_{t+h} = \\left(l_{t} + h b_{t-1}\\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{split}$",
# T="Ad"
    "$\\begin{split}
      &y_{t} = (l_{t-1} + \\phi b_{t-1})(1 + \\epsilon_t) \\\\
      &l_t = (l_{t-1} + \\phi b_{t-1})(1 + \\alpha \\epsilon_t) \\\\
      &b_t = \\phi b_{t-1} + \\beta \\mu_{y,t|t-1} \\epsilon_t \\\\
      &\\mu_{y,t|t-1} = l_{t-1} + \\phi b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_t \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} + \\phi b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} + \\alpha \\mu_{y,t|t-1} \\epsilon_t \\\\
      &b_t = \\phi b_{t-1} + \\beta \\mu_{y,t|t-1} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t|t-1} \\epsilon_t \\\\
      &\\mu_{y,t|t-1} = l_{t-1} + \\phi b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} + \\sum_{j=1}^h \\phi^j b_{t-1} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} + \\phi b_{t-1}) s_{t-m}(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} + \\phi b_{t-1} (1 + \\alpha \\epsilon_t) \\\\
      &b_t = \\phi b_{t-1} + \\beta (l_{t-1} + \\phi b_{t-1}) \\epsilon_t \\\\
      &s_t = s_{t-m}(1 + \\gamma \\epsilon_t) \\\\
      &\\mu_{y,t|t-1} = (l_{t-1} + \\phi b_{t-1}) s_{t-m} \\\\
      &\\hat{y}_{t+h} = \\left(l_{t} + \\sum_{j=1}^h \\phi^j b_t \\right) s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} = \\hat{y}_{t+h} \\text{ only for } h \\leq m
    \\end{split}$",
# T="M"
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1} (1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1} (1 + \\beta \\epsilon_t) \\\\
      &\\mu_{y,t|t-1} = l_{t-1} b_{t-1} \\\\
      &\\hat{y}_{t+h} = l_{t} b_t^h \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} b_{t-1} + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1} + \\alpha \\mu_{y,t|t-1} \\epsilon_t \\\\
      &b_t = b_{t-1} + \\beta \\frac{\\mu_{y,t|t-1}}{l_{t-1}} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t|t-1} \\epsilon_t \\\\
      &\\mu_{y,t|t-1} = l_{t-1} b_{t-1} + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^h + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1} s_{t-m} (1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1} (1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1} (1 + \\beta \\epsilon_t) \\\\
      &s_t = s_{t-m} (1 + \\gamma \\epsilon_t) \\\\
      &\\mu_{y,t|t-1} = l_{t-1} b_{t-1} s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^h s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$",
# T="Md"
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1}^\\phi (1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi (1 + \\alpha \\epsilon_t) \\\\
      &b_t = b_{t-1}^\\phi (1 + \\beta \\epsilon_t) \\\\
      &\\mu_{y,t|t-1} = l_{t-1} b_{t-1}^\\phi \\\\
      &\\hat{y}_{t+h} = l_{t} b_t^{\\sum_{j=1}^h \\phi^j} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = (l_{t-1} b_{t-1}^\\phi + s_{t-m})(1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi + \\alpha \\mu_{y,t|t-1} \\epsilon_t \\\\
      &b_t = b_{t-1}^\\phi + \\beta \\frac{\\mu_{y,t|t-1}}{l_{t-1}} \\epsilon_t \\\\
      &s_t = s_{t-m} + \\gamma \\mu_{y,t|t-1} \\epsilon_t \\\\
      &\\mu_{y,t|t-1} = l_{t-1} b_{t-1}^\\phi + s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} + s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$",
    "$\\begin{split}
      &y_{t} = l_{t-1} b_{t-1}^\\phi s_{t-m} (1 + \\epsilon_t) \\\\
      &l_t = l_{t-1} b_{t-1}^\\phi \\left(1 + \\alpha \\frac{\\epsilon_t}{s_{t-m}}\\right) \\\\
      &b_t = b_{t-1}^\\phi \\left(1 + \\beta \\frac{\\epsilon_t}{l_{t-1}s_{t-m}}\\right) \\\\
      &s_t = s_{t-m} \\left(1 + \\gamma \\frac{\\epsilon_t}{l_{t-1} b_{t-1}}\\right) \\\\
      &\\mu_{y,t|t-1} = l_{t-1} b_{t-1}^\\phi s_{t-m} \\\\
      &\\hat{y}_{t+h} = l_{t} b_{t-1}^{\\sum_{j=1}^h \\phi^j} s_{t+h-m\\lceil\\frac{h}{m}\\rceil} \\\\
      &\\mu_{y,t+h|t} \\text{ - no closed form}
    \\end{split}$")
etsMultiplicativeTable <- matrix(etsMultiplicativeTable, 5, 3, byrow=TRUE,
                           dimnames=list(c("**No trend**","**Additive trend**","**Additive damped trend**",
                                           "**Multiplicative trend**","**Multiplicative damped trend**"),
                                         c("N","A","M")))
kableTable <- kableExtra::kable(etsMultiplicativeTable, escape=FALSE, caption="Multiplicative error ETS models",
                                col.names=c("Nonseasonal","Additive seasonality","Multiplicative seasonality"))
kable_styling(kableTable, font_size=12, protect_latex=TRUE)
```

The formulae summarised above explain the models underlying potential data, but when it comes to their construction and estimation, the $\epsilon_t$ is substituted by the estimated $e_t$ (which is calculated differently depending on the error type), and time series components and smoothing parameters are also substituted by their estimated analogues (e.g. $\hat{\alpha}$ instead of $\alpha$).

Although there are 30 potential ETS models, not all of them are stable So, Rob Hyndman has reduced the pool of models under consideration in the `ets()` function of `forecast` package to the following 19: ANN, AAN, AAdN, ANA, AAA, AAdA, MNN, MAN, MAdN, MNA, MAA, MAdA, MNM, MAM, MAdM, MMN, MMdN, MMM, MMdM. In addition, the multiplicative trend models are difficult and are unstable in cases of data with outliers, so they are switched off in the `ets()` function by default, which reduces the pool of models further to the first 15.


## ETS assumptions, estimation and selection
There are several assumptions that need to hold for the conventional ETS models in order for them to be used in practice appropriately. Some of them have already been discussed in [one of the previous sections](#assumptions), and we will not discuss them here again. What is important in our context is that the conventional ETS assumes that the error term $\epsilon_t$ follows normal distribution with zero mean and variance $\sigma^2$. As discussed [earlier](#distributions), normal distribution is defined for positive, negative and zero values. This is not a big deal for additive models, which assume that the actual value can be anything. As for the multiplicative models this will work, when we deal with high level positive data (e.g. thousands of units): the variance of the error term will be small enough for the $\epsilon_t$ not to become less than one. However, if the level of the data is low, then the variance of the error term can be large enough for the normally distributed error to cover negative values, less than one. This implies that the error term $1+\epsilon_t$ can become negative, and the model will break. This is a potential flaw in the conventional ETS model with the multiplicative error term. So, what the conventional multiplicative error ETS model assumes in fact is that **the data we work with is strictly positive and has high level values**.

Based on the assumption of normality of error term, the ETS model can be estimated via the maximisation of likelihood, which is equivalent to the minimisation of the mean squared forecast error $e_t$. Note that in order to apply the ETS models to the data, we also need to know the initial values of components, $\hat{l}_0, \hat{b}_0, \hat{s}_{-m+2}, \hat{s}_{-m+3}, \dots, \hat{s}_{0}$. The conventional approach is to estimate these values together with the smoothing parameters during the maximisation of likelihood. As a result, the optimisation might involve a large number of parameters. In addition, the variance of the error term is considered as an additional parameter in the maximum likelihood estimation, so the number of parameters for different models is (here "*" stands for any type):

1. ETS(\*,N,N) - 3 parameters: $\hat{l}_0$, $\hat{\alpha}$ and $\hat{\sigma}^2$;
2. ETS(\*,\*,N) - 5 parameters: $\hat{l}_0$, $\hat{b}_0$, $\hat{\alpha}$, $\hat{\beta}$ and $\hat{\sigma}^2$;
3. ETS(\*,\*d,N) - 6 parameters: $\hat{l}_0$, $\hat{b}_0$, $\hat{\alpha}$, $\hat{\beta}$, $\hat{\phi}$ and $\hat{\sigma}^2$;
4. ETS(\*,N,\*) - 3+m-1 parameters: $\hat{l}_0$, $\hat{s}_{-m+2}, \hat{s}_{-m+3}, \dots, \hat{s}_{0}$, $\hat{\alpha}$, $\hat{\gamma}$ and $\hat{\sigma}^2$;
5. ETS(\*,\*,\*) - 5+m-1 parameters: $\hat{l}_0$, $\hat{b}_0$, $\hat{s}_{-m+2}, \hat{s}_{-m+3}, \dots, \hat{s}_{0}$, $\hat{\alpha}$, $\hat{\beta}$, $\hat{\gamma}$ and $\hat{\sigma}^2$;
6. ETS(\*,\*d,\*) - 6+m-1 parameters: $\hat{l}_0$, $\hat{b}_0$, $\hat{s}_{-m+2}, \hat{s}_{-m+3}, \dots, \hat{s}_{0}$, $\hat{\alpha}$, $\hat{\beta}$, $\hat{\gamma}$, $\hat{\phi}$ and $\hat{\sigma}^2$.

Note that in case of seasonal models we typically make sure that the initial seasonality indices are normalised, so we only need to estimate $m-1$ of them, the last one is calculated based on the linear combination of the others.

Finally, when it comes to the selection of the most appropriate model, the conventional approach involves the application of all models to the data and then selecting the most appropriate of them based on [an information cretiria](#modelSelection). In case of the conventional ETS model, this relies on the likelihood value of normal distribution, used in the estimation of the model.

## State space form of ETS
One of the main advantages of the ETS model is its state space form, which gives it the flexibility. We would need to revert to linear algebra in this section in order to understand how any ETS model can be presented in a compact state space form.
