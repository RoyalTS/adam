# Exponential smoothing methods and conventional ETS

## Simple Exponential Smoothing
We start our discussion of exponential smoothing with the original Simple Exponential Smoothing (SES) forecasting method, which was formulated by [@Brown1956]:
\begin{equation}
  \hat{y}_{t+1} = \hat{\alpha} {y}_{t} + (1 - \hat{\alpha}) \hat{y}_{t},
  (\#eq:BrownMethod)
\end{equation}
where $\hat{\alpha}$ is the smoothing parameter, defined by analyst and which is typically restricted with (0, 1) region (this region is actually arbitrary and we will see later what is the correct one). This is one of the simplest forecasting methods, and the smoothing parameter in it is typically interpretted as a weight between the actual value and the one-step-ahead predicted one. If the smoothing parameter is close to zero, then more weight is given to the previous fitted value $\hat{y}_{t}$ and the new information is neglected. When it is close to one, then mainly the actual value ${y}_{t}$ is taken into account. By changing the smoothing parameter value, the forecaster can decide how to approximate the data and filter out the noise.

Also, notice that this is a recursive method, meaning that there needs to be some starting point $\hat{y}_1$ in order to apply \@ref(eq:BrownMethod) to the existing data. Different initialisation and estimation methods for SES have been discussed in the literature, but the sttate of the art one is to estimate $\hat{\alpha}$ and $\hat{y}_{1}$ together by minimising some loss function. Typically [MSE](#errorMeasures) is used as one, minimising the one step ahead forecast error.

Here is an example of how this method works on different time series. We start with generating a stationary series and using `es()` function from `smooth` package. Although it implements the ETS model, we will see later the connection between SES and ETS(A,N,N). We start with the stationary time series and $\hat{\alpha}=0$:
```{r SESExample1}
y <- rnorm(100,100,10)
ourModel <- es(y, model="ANN", h=10, persistence=0)
plot(ourModel, 7, main=paste0("SES with alpha=",ourModel$persistence))
```

The SES works well in this case, capturing the deterministic level of the series and filtering out the noise. In this case, it works like a global average applied to the data. As mentioned before, the method is flexible, so if we have a level shift in the data and increase the smoothing parameter, it will adapt and get to the new level. Here is an example:

```{r SESExample2}
y <- c(rnorm(50,100,10),rnorm(50,130,10))
ourModel <- es(y, model="ANN", h=10, persistence=0.1)
plot(ourModel, 7, main=paste0("SES with alpha=",ourModel$persistence))
```

With $\hat{\alpha}=0.1$, it manages to get to the new level, but now the method starts adapting to noise a little bit - it follows the peaks and troughs and repeats them, but with much smaller magnitude. If we increase the smoothing parameter, it will react to the changes much faster, but it will also react more to noise:
```{r echo=FALSE}
par(mfcol=c(3,1))
ourModel <- es(y, model="ANN", h=10, persistence=0.2)
plot(ourModel, 7, main=paste0("SES with alpha=",ourModel$persistence))
ourModel <- es(y, model="ANN", h=10, persistence=0.3)
plot(ourModel, 7, main=paste0("SES with alpha=",ourModel$persistence))
ourModel <- es(y, model="ANN", h=10, persistence=0.5)
plot(ourModel, 7, main=paste0("SES with alpha=",ourModel$persistence))
```

If we set $\hat{\alpha}=1$, we will end up with Naive forecasting method, which is not appropriate for our example:
```{r echo=FALSE}
par(mfcol=c(1,1))
ourModel <- es(y, model="ANN", h=10, persistence=1)
plot(ourModel, 7, main=paste0("SES with alpha=",ourModel$persistence))
```

So, when working with SES, we need to make sure that the reasonable smoothing parameter is selected. This can be done automatically via minimising the MSE:
```{r SESExample3}
ourModel <- es(y, model="ANN", h=10, loss="MSE")
plot(ourModel, 7, main=paste0("SES with alpha=",round(ourModel$persistence,3)))
```

This approach won't guarantee that we will get the most appropriate $\hat{\alpha}$, but it has been shown in the literature that the optimisation of smoothing parameter on average leads to improvements in terms of forecasting.

An alternative form of SES is known as error correction form and involves some simple permutations, taking that $e_t=y_t-\hat{y}_t$ is the one step ahead forecast error:
\begin{equation}
  \hat{y}_{t+1} = \hat{y}_{t} + \hat{\alpha} e_{t}.
  (\#eq:SESErrorCorrection)
\end{equation}
In this form, the smoothing parameter $\hat{\alpha}$ regulates how much the model reacts to the forecast error. In this interpretation it no longer needs to be restricted with (0, 1) region, but we would still typically want it to be closer to zero, in order to filter out the noise, not to adapt to it.

As you see, this is a very simple method. It is easy to explain it to practitioners and it is very easy to implement in practice. However, this is just a [forecasting method](#intro), so it just gives a way of generating point forecasts, but does not explain where the error comes from and how to generate prediction intervals.

## SES and ETS(A,N,N)
There have been several tries to develop statistical models, underlying SES, and we know now that it has underlying ARIMA(0,1,1), local level MSOE (Multiple Source of Error) model [@Muth1960] and SSOE (Single Source of Error) model [@Snyder1985]. According to [@Hyndman2002], the ETS(A,N,N) model also underlies the SES method. It can be formulated in the following way:
\begin{equation}
  \begin{split}
    y_{t} &= l_{t-1} + \epsilon_t \\
    l_t &= l_{t-1} + \alpha \epsilon_t
  \end{split} ,
  (\#eq:ETSANN)
\end{equation}
where, as we know from [the previous section](#tsComponents), $l_t$ is the level of the data, $\epsilon_t$ is the error term and $\alpha$ is the smoothing parameter. Note that we use $\alpha$ without the "hat" symbol, which implies that there is a "true" value of the parameter (which could be obtained if we had all the data in the world or just knew it for some reason). It is easy to show that ETS(A,N,N) underlies SES. In order to see this, we need to take move towards estimation phase and use $\hat{l}_{t-1}=l_{t-1}$ and move to estimates $\hat{\alpha}$ and $e_t$ (the estimate of the error term $\epsilon_t$):
\begin{equation}
  \begin{split}
    y_{t} &= \hat{l}_{t-1} + e_t \\
    \hat{l}_t &= \hat{l}_{t-1} + \hat{\alpha} e_t
  \end{split} ,
  (\#eq:ETSANNEstimation)
\end{equation}
and also take that $\hat{y}_t=l_{t-1}$:
\begin{equation}
  \begin{split}
    y_{t} &= \hat{y}_{t} + e_t \\
    \hat{y}_{t} &= \hat{y}_{t-1} + \hat{\alpha} e_{t-1}
  \end{split} .
  (\#eq:ETSANNEstimation3)
\end{equation}
Inserting the second equation in the first one and substituting $y_t$ with $\hat{y}_t+e_t$ we get:
\begin{equation}
    \hat{y}_t+e_t = \hat{y}_{t-1} + \hat{\alpha} e_{t-1} + e_t ,
  (\#eq:ETSANNEstimation4)
\end{equation}
cancelling out $e_t$ and shifting everything by one step ahead, we obtain the error correction form \@ref(eq:SESErrorCorrection) of SES.

But now, the main benefit of having the model \@ref(eq:ETSANN) instead of just the method \@ref(eq:SESErrorCorrection) is in having a flexible framework, which allows adding other components, selecting the most appropriate ones, estimating parameters in a [consistent](#intro) way, producing prediction intervals etc.

In order to see the data that corresponds to the ETS(A,N,N) we can use `sim.es()` function from smooth package. Here are several examples with different smoothing parameters:
```{r}
x <- vector("list",6)
initial <- 1000
meanValue <- 0
sdValue <- 20
alphas <- c(0.1,0.3,0.5,0.75,1,1.5)
for(i in 1:length(alphas)){
  x[[i]] <- sim.es("ANN", 120, 1, 12, persistence=alphas[i], initial=initial, mean=meanValue, sd=sdValue)
}

par(mfcol=c(3,2))
for(i in 1:6){
  plot(x[[i]], main=paste0("alpha=",x[[i]]$persistence), ylim=initial+c(-500,500))
}
```

This simple simulation shows that the higher $\alpha$ is, the higher variability is in the data and less predictable the data becomes. This is related with the higher values of $\alpha$, the level changes faster, also leading to the increased uncertainty about the future values of the level in the data.

When it comes to the application of this model to the data, the point forecast corresponds to the conditional h steps ahead mean and is equal to the last observed level:
\begin{equation}
    \mu_{y,t+h|t} = l_{t} ,
  (\#eq:ETSANNForecast)
\end{equation}
this holds because it is [assumed](#assumptions) that $\text{E}(\epsilon_t)=0$, which implies that the conditional h steps ahead expectation of the level in the model is $\text{E}(l_{t+h}|t)=l_t+\alpha\sum_{j=1}^{h-1}\epsilon_{t+j} = l_t$.

Here is an example with automatic parameter estimation in ETS(A,N,N) using `es()` function from `smooth` package:
```{r ETSANNExample}
x <- sim.es("ANN", 120, 1, 12, persistence=0.3, initial=1000)
ourModel <- es(x$data, "ANN", h=12, interval=TRUE, holdout=TRUE, silent=FALSE)
ourModel
```

As we see, the true smoothing parameter is 0.3, but the estimated one is not exactly 0.3, which is expected, because we deal with an in-sample estimation. Also, notice that with such a high smoothing parameter, the prediction interval is widening with the increase of the forecast horizon. If the smoothing parameter would be lower, then the bounds would not increase, but this might not reflect the uncertainty about the level correctly. Here is an example with $\alpha=0.01$:
```{r ETSANNExamplealpha0.1}
ourModel <- es(x$data, "ANN", h=12, interval=TRUE, holdout=TRUE, silent=FALSE, persistence=0.01)
```
In this case, the prediction interval is wider than needed and the forecast is biased - the model does not keep up to the fast changing time series. So, it is important to correctly estimate the smoothing parameters not only to approximate the data, but also to produce less biased point forecast and more appropriate prediction interval.


## Other exponential smoothing methods and ETS
There are other exponential smoothing, which include more components, as discussed in [the previous section](#tsComponents). This includes but is not restricted with: Holt's [@Holt2004b] (originally proposed in 1957), Holt-Winter's [@Winters1960], multiplicative trend [@Pegels1969], Damped trend (originally proposed by @Roberts1982 and then picked up by @Gardner1985a), Damped trend Holt-Winters [@Gardner1989] and damped multiplicative trend methods [@Taylor2003]. We will not disuss them here one by one, as we will not use them further in this textbook. More importantly, all of them have underlying ETS models, so we will focus on them instead.

We already understand that there can be different components in time series and that they can interact with each other either in an additive or a multiplicative way, which gives us the aforementioned taxonomy. The equations discussed in [the previous section](#tsComponents) represent so called "measurement" or "observation" equations of the ETS models. But we should also reflect the potential change in components over time, in a similar manner done in the second equation of \@ref(eq:ETSANN). Here are several examples of ETS models with several components.

### ETS(A,A,N)
This is also sometimes known as local trend model and is formulated as ETS(A,N,N), but with addition of the trend equation. It underlies Holt's method:
\begin{equation}
  \begin{split}
    y_{t} &= l_{t-1} + b_{t-1} + \epsilon_t \\
    l_t &= l_{t-1} + b_{t-1} + \alpha \epsilon_t \\
    b_t &= b_{t-1} + \beta \epsilon_t
  \end{split} ,
  (\#eq:ETSAAN)
\end{equation}
where $\beta$ is the smoothing parameter for the trend component. It has a similar idea as ETS(A,N,N): the states evolve over time, and the speed of their change depends on the values of $\alpha$ and $\beta$. 

Here is an example of the data that corresponds to the ETS(A,A,N) model:
```{r}
x <- sim.es("AAN", 120, 1, 12, persistence=c(0.3,0.1), initial=c(1000,20), mean=0, sd=20)
plot(x)
```

As you might notice, the trend is not deterministic in this model: both the intercept and the slope change over time. The higher the smoothing parameters are, the more uncertain it is, what the level and the slope will be, thus higher the uncertainty about the future values is.

The point forecast h steps ahead from this model is a straight line with a slope $b_t$:
\begin{equation}
    \mu_{y,t+h|t} = l_{t} + h b_t.
  (\#eq:ETSAANForecast)
\end{equation}
This becomes apparent if one takes the conditional expectations E$(l_{t+h}|t)$ and E$(b_{t+h}|t)$ in the second and third equations of \@ref(eq:ETSAAN).

### ETS(A,Ad,N)
This is the model that underlies Damped trend method [@Roberts1982]:
\begin{equation}
  \begin{split}
    y_{t} &= l_{t-1} + \phi b_{t-1} + \epsilon_t \\
    l_t &= l_{t-1} + \phi b_{t-1} + \alpha \epsilon_t \\
    b_t &= \phi b_{t-1} + \beta \epsilon_t
  \end{split} ,
  (\#eq:ETSAAdN)
\end{equation}
where $\phi$ is the dampening parameter, typically lying between 0 and 1. If it is equal to zero, then the model \@ref(eq:ETSAAdN) reduces to \@ref(eq:ETSANN). If it is equal to one, then it becomes equivalent to \@ref(eq:ETSAAN). The dampening parameter slows down the trend, making it non-linear. The point forecast from this model is a bit more complicated:
\begin{equation}
    \mu_{y,t+h|t} = l_{t} + \sum_{j=1}^h \phi^j b_t.
  (\#eq:ETSAANForecast)
\end{equation}

### ETS(A,A,A)



## Additional information

If you want to know more about the history of exponential smoothing, how it was developed and what papers contributed towards the development of the field, then the reviews of [@Gardner1985] and [@Gardner2006] are the great places to start with. They summmarise all the progress in the area of exponential smoothing up until 1985 and then until 2006.
