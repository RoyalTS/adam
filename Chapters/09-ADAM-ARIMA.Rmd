# ADAM ARIMA {#ADAMARIMA}
Another important dynamic element in ADAM is ARIMA model [developed originally by @Box1976]. ARIMA stands for "AutoRegressive Integrated Moving Average", although the name does not tell much on its own and needs additional explanation, which will be provided in the next sections. The main idea of the model is that the data might have dynamic relations over time, where the new values depend on the values on the previous observations. This becomes more obvious in case of engineering systems and modelling phisical processes. For example, @Box1976 give an example of a series of CO$_2$ output of a furnace, when the input gas rate changes. In this case, the elements of ARIMA process are natural, as the CO$_2$ cannot just drop to zero, when the gas is switched off - it will leave the furnace, in reducing quantity over time (i.e. leaving $\phi_1\times100\%$ of CO$_2$ in the next minute, where $\phi_1$ is a parameter in the model). Another example, where AR processes are natural is the temperature in the room, measured with 5 minutes intervals. In this case the temperature at 5:30pm will depend on the one at 5:25pm (if the temperature outside the room is lower, then it will go down slightly due to the loss of heat). So, in these examples, ARIMA model can be considered as a [true model](#intro), but when it comes to time series from the social or business domain, it becomes very difficult to motivate the usage of ARIMA from the from the modelling point of view. For example, the demand on products does not reproduce itself and in real live does not depend on the demand on previous observations, unless we are talking about a repetitive purchases by the same group of consumers. So, if we construct ARIMA for such process, we are closing eyes on the fact that the observed time series relations in the data are most probably spurious. At best, ARIMA in this case can be considered as a very crude approximation of a complex true process (demand is typically influenced by price changes, consumer behaviour and promotional activities). So, whenever we work with ARIMA models in social or business domain, we should keep in mind that they are wrong even from the philosophical point of view. Nevertheless, they still can be useful, which is why we discuss them in this chapter.


## Introduction to ARIMA
ARIMA contains several elements:

1. AR(p) - the AutoRegressive part, showing how the variable is impacted by its values on the previous observations. It contains $p$ lags. For example, the quantity of the liquid in a vessel with an opened tap on some observation will depend on the quantity on the previous steps. This analogy explains the idea of AR part of the model;
2. I(d) - the number of differences $d$ taken in the model (I stands for "Integrated). Working with differences rather than with the original data means that we deal with changes and rates of changes, not with just values. Technically, differences are needed in order to make data stationary (i.e. with fixed expectation and variance, although there are different definitions of the term *stationarity*);
3. MA(q) - the Moving Average component, explaining how the variable is impacted by the previous white noise. It contains $q$ lags. Once again, in technical systems, the idea that the random error can impact the value, has a relatively simple explanation. For example, when the liquid drips out of a vessel, we might not be able to predict the air fluctations, which would impact the flow and could be perceived as elements of random noise. This randomness might in turn impact the quantity of liquid in a vessel on a next observation, thus introducing the MA elements in the model.

I intentionally do not provide ARIMA examples from the demand forecasting area, as these are much more difficult to motivate and explain than the examples from the more technical areas.

Before we continue our discussion, we should define the term **stationarity**. There are two definitions in the literature, one refers to "strict stationarity", while the other refers to the "weak stationarity":

- Time series is said to be **weak stationary**, when its conditional expectation and variance are constant and the variance is finite for all times;
- Time series is **strong stationary**, when its unconditional joint probability distribution does not change over time. This automatically implies that all its moments are constant (i.e. the process is also weak stationary).

The stationarity is very important in ARIMA context and also plays important role in regression analysis. If the series is not stationary, then it might be difficult to estimate its moments correctly using the conventional methods. In this case, the series is somehow transformed in order to make sure that the moments are finite and constant (e.g. take logarithms or do Box-Cox transform, take differences or detrend the series). Note that in contrast with ARIMA, the ETS models are almost always all non-stationary and do not require for the series to be stationary. We will see the connection between the two approaches later in this chapter.


### AR(p) {#ADAMAR}
We start with a simple AR(1) model, which is written as:
\begin{equation}
  {y}_{t} = \phi_1 y_{t-1} + \epsilon_t ,
  (\#eq:ADAMARIMA100Example)
\end{equation}
where $\phi_1$ is the parameter of the model. This formula tells us that the value on the previous observation is carried out to the new one in the proportion of $\phi_1$. Typically, the parameter $\phi_1$ is restricted with the region (-1, 1), in order to make the model stationary, but very often in real life $\phi_1$ actually lies in (0, 1) region. If the parameter is equal to 1, then the model becomes equivalent to Random Walk.

The forecast trajectory (conditional expectation several steps ahead) of this model would typically correspond to the exponentially declining curve. Here is a simple example in R of a very basic forecast from AR(1) with $\phi_1=0.9$:
```{r}
x <- vector("numeric", 20)
x[1] <- 100
phi <- 0.9
for(i in 2:20){
    x[i] <- phi * x[i-1]
}
plot(x, type="l", xlab="horizon")
```

If for some reason we get $\phi_1>1$, then the trajectory corresponds to exponential increase, becoming explosive, implying non-stationary behaviour. The model in this case becomes very difficult to work with,  even if the parameter is close to one. So it is typically advised to restrict the parameter with stationarity region (we will discuss this in more detail later in this chapter).

In general, it is possible to imagine the situation, when the value at the moment of time $t$ would depend on several previous values, so the model AR(p) can be written as:
\begin{equation}
  {y}_{t} = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \epsilon_t ,
  (\#eq:ADAMARIMAp00Example)
\end{equation}
where $\phi_i$ is the parameters for the $i$-th lag of the model. So, the model assumes that the data on the recent observations is influenced by the $p$ previous observations. The more lags we introduce in the model, the more complicated the forecasting trajectory becomes, potentially introducing harmonic behaviour. Here is an example of AR(3) model ${y}_{t} = 0.9 y_{t-1} -0.7 y_{t-2} + 0.6 y_{t-3} + \epsilon_t$:
```{r}
x <- vector("numeric", 30)
x[1] <- 100
x[2] <- 75
x[3] <- 30
phi <- c(0.9,-0.7,0.6)
for(i in 4:30){
    x[i] <- phi[1] * x[i-1] + phi[2] * x[i-2] + phi[3] * x[i-3]
}
plot(x, type="l", xlab="horizon")
```

No matter what the forecast trajectory of AR model is, it will asymptotically converge to zero, as long as the model is stationary.

### MA(q)
Before discussing the "Moving Averages" model, we should acknowledge that the name is quite misleading, and that the model has *nothing to do* with Centred Moving Averages used in time series decomposition or Simple Moving Averages (average of several concequitive observation). The idea of the simplest MA(1) model can be summarised in the following mathematical way:
\begin{equation}
  {y}_{t} = \theta_1 \epsilon_{t-1} + \epsilon_t ,
  (\#eq:ADAMARIMA001Example)
\end{equation}
where $\theta_1$ is the parameter of the model, typically lying between (-1, 1), showing what part of the error is carried out to the next observation. Because of the conventional assumption that the error term has a zero mean ($\mathrm{E}(\epsilon_{t})=0$), the forecast trajectory of this model is just a straight line coinsiding with zero starting from the $h=2$. For the one step ahead forecast we have:
\begin{equation}
  \mathrm{E}({y}_{t+1}|t) = \theta_1 \mathrm{E}(\epsilon_{t}|t) + \mathrm{E}(\epsilon_{t+1}|t) = \theta_1 \epsilon_{t}.
  (\#eq:ADAMARIMA001ExampleForecast)
\end{equation}
But starting from $h=2$ there are no observable error terms $\epsilon_t$, so all the values past that are equal to zero:
\begin{equation}
  \mathrm{E}({y}_{t+2}|t) = \theta_1 \mathrm{E}(\epsilon_{t+1}|t) + \mathrm{E}(\epsilon_{t+2}|t) = 0.
  (\#eq:ADAMARIMA001ExampleForecastInSample)
\end{equation}
So, the forecast trajectory for MA(1) model converges to zero, when $h>1$.

More generally, MA(q) model is written as:
\begin{equation}
  {y}_{t} = \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q} + \epsilon_t ,
  (\#eq:ADAMARIMA00qExample)
\end{equation}
where $\theta_i$ is the parameters for the $i$-th lag of the error term, which are typically restricted with the so called invertibility region (discussed in the next section). In this case, the model assumes that the recent observation is influenced by several errors on previous observations (your mistakes in the past will haunt you in the future). The more lags we introduce, the more complicated the model becomes. As for the forecast trajectory, it will reach zero, when $h>q$.

### ARMA(p,q)
Connection the models \@ref(eq:ADAMARIMAp00Example) and \@ref(eq:ADAMARIMA00qExample), we get the more complicated model, ARMA(p,q):
\begin{equation}
  {y}_{t} = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q} + \epsilon_t ,
  (\#eq:ADAMARIMAp0q)
\end{equation}
which has the properties of the two models discussed above. The forecast trajectory from this model will have a combination of trajectories for AR and MA for $h \leq q$ and then will correspond to AR(p) for $h>q$.

In order to simplify the work with ARMA models, the equation \@ref(eq:ADAMARIMAp0q) is typically rewritten, by moving all terms with $y_t$ to the left hand side:
\begin{equation}
  {y}_{t} - \phi_1 y_{t-1} - \phi_2 y_{t-2} - \dots - \phi_p y_{t-p} = \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q} + \epsilon_t .
  (\#eq:ADAMARIMAp0qLeft)
\end{equation}
Furthermore, in order to make this even more compact, the backshift operator B is introduced, which just shows by how much the subscript of the variable is shifted back in time:
\begin{equation}
  {y}_{t} B^i = {y}_{t-i}.
  (\#eq:backshiftOperator)
\end{equation}
Using \@ref(eq:backshiftOperator), the ARMA model can be written as:
\begin{equation}
  {y}_{t} (1 - \phi_1 B - \phi_2 B^2 - \dots - \phi_p B^p) = \epsilon_t (1 + \theta_1 B + \theta_2 B^2 + \dots + \theta_q B^q) .
  (\#eq:ADAMARIMAp0qCompacter)
\end{equation}
Finally, we can also introduce the AR and MA polynomial functions to make the model even more compact:
\begin{equation}
\begin{aligned}
  & \varphi(p) = 1 - \phi_1 B - \phi_2 B^2 - \dots - \phi_p B^p \\ 
  & \vartheta(q) = 1 + \theta_1 B + \theta_2 B^2 + \dots + \theta_q B^q .
\end{aligned}
  (\#eq:ARMAPolynomials)
\end{equation}
Inserting the functions \@ref(eq:ARMAPolynomials) in \@ref(eq:ADAMARIMAp0qCompacter) leads to the compact presentation of ARMA model:
\begin{equation}
  {y}_{t} \varphi(p) = \epsilon_t \vartheta(q) .
  (\#eq:ADAMARIMAp0qCompact)
\end{equation}
The model \@ref(eq:ADAMARIMAp0qCompact) can be considered as a compact form of \@ref(eq:ADAMARIMAp0q). It is more difficult to understand and interpret, but easier to work with from mathematical point of view. In addition, this form permits introducing additional elements, which will be discussed later in this chapter.

Coming back to the ARMA model \@ref(eq:ADAMARIMAp0q), we might notice, that it assumes convergence to zero, the speed of which is regulated via the parameters. In fact, this implies that the data has the mean of zero, and ARMA becomes useful, when the data is somehow pre-processed, so that it is stationary and varies around zero. This means that if you work with non-stationary and / or with non-zero mean data, the pure AR / MA or ARMA will be inappropriate - some prior transformations are in order.

### ARMA with constant
One of the simpler ways to deal with the issue with zero forecasts is to introduce the constant (or intercept) in ARMA:
\begin{equation}
  {y}_{t} = a_0 + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_p \epsilon_{t-p} + \epsilon_t 
  (\#eq:ADAMARIMAp0qExample)
\end{equation}
or
\begin{equation}
  {y}_{t} \varphi(p) = a_0 + \epsilon_t \vartheta(q) ,
  (\#eq:ADAMARIMAp0qCompact)
\end{equation}
where $a_0$ is the constant parameter, which in this case also works as the unconditional mean of the series. The forecast trajectory in this case would converge to $a_0$ instead of zero, but with some minor differences from the ARMA without constant. For example, in case of ARMA(1,1) with constant we will have:
\begin{equation}
  {y}_{t} = a_0 + \phi_1 y_{t-1} + \theta_1 \epsilon_{t-1} + \epsilon_t .
  (\#eq:ADAMARIMA101ConstExample01)
\end{equation}
The conditional expectation of $y_{t+h}$ for $h=1$ and $h=2$ can be written as (based on the discussions in previous sections):
\begin{equation}
\begin{aligned}
  & \mathrm{E}({y}_{t+1}|t) = a_0 + \phi_1 y_{t} + \theta_1 \epsilon_{t} \\
  & \mathrm{E}({y}_{t+2}|t) = a_0 + \phi_1 \mathrm{E}(y_{t+1}|t) = a_0 + \phi_1 a_0 + \phi_1^2 y_{t} + \phi_1 \theta_1 \epsilon_t
\end{aligned} ,
  (\#eq:ADAMARIMA101ConstExampleForecasth1)
\end{equation}
or in general for some horizon $h$:
\begin{equation}
  \mathrm{E}({y}_{t+h}|t) = \sum_{j=1}^h a_0\phi_1^{j-1} + \phi_1^h y_{t} + \phi_1^{h-1} \theta_1 \epsilon_{t} .
  (\#eq:ADAMARIMA101ConstExampleForecast)
\end{equation}
So, the forecast trajectory from this model dampens out, similar to the [ETS(A,Ad,N)](#ETSAAdN) model, and the rate of dampening is regulated by the value of $\phi_1$. The more complicated ARMA(p,q) models with p>1 will have more complicated trajectories with potential harmonics.

Alternatively, each actual value of $y_t$ can be centred via $y^\prime_t = y_t - \bar{y}$, making sure that the mean of $y^\prime_t$ is zero and ARMA can be applied to the $y^\prime_t$ data instead of $y_t$. However, this approach introduces additional steps, but the result on stationary data is typically the same.

### I(d)
Based on the previous discussion, we can conclude that ARMA cannot be applied to non-stationary data. So, if we deal with one, we need to make it stationary somehow. The convetional way of doing that is by taking differences of the data. The logic behind this is straight forward: if the data is not stationary, then the mean somehow changes over time. This can be, for example, due to a trend in the data. In this case we should be taking about the change of variable $y_t$ rather than the variable itself. So we should work on the following data instead:
\begin{equation}
  \Delta_{y,t} = y_t - y_{t-1} = y_t (1 - B),
  (\#eq:ADAMARIMADifferencesFirst)
\end{equation}
if the differences have constant mean. The simplest model with differences is I(1), which is also known as the **Random walk**:
\begin{equation}
  \Delta_{y,t} = \epsilon_t,
  (\#eq:ADAMARIMARandomWalk)
\end{equation}
which can be reformulated in a simpler, more interpretable form by inserting \@ref(eq:ADAMARIMADifferencesFirst) in \@ref(eq:ADAMARIMARandomWalk) and regrouping elements:
\begin{equation}
  y_t = y_{t-1} + \epsilon_t.
  (\#eq:ADAMARIMARandomWalk02)
\end{equation}
The model \@ref(eq:ADAMARIMARandomWalk02) can also be perceived as AR(1) with $\phi_1=1$. This is a non-stationary model, meaning that the unconditional mean of $y_t$ is not constant. The forecast from this model corresponds to the Na\"ive method with straight line equal to the last observed actual value (again, assuming that $\mathrm{E}(\epsilon_{t})=0$ and that [other basic assumptions](#assumptions) hold):
\begin{equation}
  \mathrm{E}(y_{t+h}|t) = \mathrm{E}(y_{t+h-1}|t) + \mathrm{E}(\epsilon_{t+h}|t) = y_{t} .
  (\#eq:ADAMARIMARandomWalkForecast)
\end{equation}

Another simple model that relies on differences of the data is called **Random Walk with drift** and is formulated by adding constant $a_0$ to the right hand side of equation \@ref(eq:ADAMARIMARandomWalk):
\begin{equation}
  \Delta_{y,t} = a_0 + \epsilon_t.
  (\#eq:ADAMARIMARandomWalkWithDrift)
\end{equation}
This model has some similarities with the global level model, which is formulated via the actual value rather than differences:
\begin{equation}
  {y}_{t} = a_0 + \epsilon_t.
  (\#eq:ADAMARIMAGlobalMean)
\end{equation}
Using a similar regrouping as with the Random Walk, we can obtain a simpler form of \@ref(eq:ADAMARIMARandomWalkWithDrift):
\begin{equation}
  y_t = a_0 + y_{t-1} + \epsilon_t.
  (\#eq:ADAMARIMARandomWalkWithDrift02)
\end{equation}
which is, again, equivalent to AR(1) model with $\phi_1=1$, but this time with a constant. The term "drift" appears because $a_0$ acts as an additional element, showing what the tendecy in the data will be: if it is positive, the model will exhibit positive trend, if it is negative, the trend will be negative. This can be seen for the conditional mean, for example, for the case of $h=2$:
\begin{equation}
  \mathrm{E}(y_{t+2}) = \mathrm{E}(a_0) + \mathrm{E}(y_{t+1}) + \mathrm{E}(\epsilon_{t+2}) = a_0 + \mathrm{E}(a_0 + y_t + \epsilon_t) + \mathrm{E}(\epsilon_{t+2}) = 2 a_0 + y_t .
  (\#eq:ADAMARIMARandomWalkWithDriftForecasth2)
\end{equation}

In a similar manner we can also introduce second differences of the data (differences of differences) if we suspect that the change of variable over time is not stationary, which would be written as:
\begin{equation}
  \Delta^2_{y,t} = \Delta_{y,t} - \Delta_{y,t-1} = y_t - y_{t-1} - y_{t-1} + y_{t-2},
  (\#eq:ADAMARIMADifferencesSecond)
\end{equation}
which can also be written in a form using backshift operator:
\begin{equation}
  \Delta^2_{y,t} = y_t(1 - 2B + B^2) = y_t (1-B)^2.
  (\#eq:ADAMARIMADifferencesSecondBackshift)
\end{equation}
In fact, we can introduce higher level differences if we want (but typically we should not) based on the idea of \@ref(eq:ADAMARIMADifferencesSecondBackshift):
\begin{equation}
  \Delta^d_{y,t} = y_t (1-B)^d.
  (\#eq:ADAMARIMADifferencesSecondBackshift)
\end{equation}
Based on that, the I(d) model is formualted as:
\begin{equation}
  \Delta^d_{y,t} = \epsilon_t.
  (\#eq:ADAMARIMA0d0)
\end{equation}

### ARIMA(p,d,q)
Finally, having made the data stationary via the differences, we can introduce ARMA elements \@ref(eq:ADAMARIMAp0qCompact) to it which would be done on the differenced data, instead of the original $y_t$:
\begin{equation}
  \Delta^d_{y,t} \varphi(p) = \epsilon_t \vartheta(q) ,
  (\#eq:ADAMARIMApdqCompact)
\end{equation}
or in a more general form \@ref(eq:ADAMARIMAp0qCompacter) with \@ref(eq:ADAMARIMADifferencesSecondBackshift):
\begin{equation}
  y_t (1-B)^d (1 - \phi_1 B - \phi_2 B^2 - \dots - \phi_p B^p) = \epsilon_t (1 + \theta_1 B + \theta_2 B^2 + \dots + \theta_q B^q),
  (\#eq:ADAMARIMApdq)
\end{equation}
which is ARIMA(p,d,q) model. This model allows producing trends with some values of differences and also inherits the trajectories from both AR(p) and MA(q). This implies that the point forecasts from the model can exhibit quite complicated trajectories, depending on the values of parameters of the model.

The model \@ref(eq:ADAMARIMApdq) is difficult to interpret in a general form, but opening the brackets and moving all elements but $y_t$ to the right hand side typically helps in understanding of each specific model.

