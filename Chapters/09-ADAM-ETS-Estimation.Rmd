# Estimation of ADAM ETS {#ADAMETSEstimation}

Now that we have discussed the properties and issues of some of ETS models, we need to understand how to estimate them. As mentioned earlier, when we apply a model to the data, we assume that it is suitable and see how it fits the data and produces forecasts. In this case all the parameters in the model are substituted by the estimated ones (observed in sample) and the error term becomes an estimate of the true one. In general this means that the state space model \@ref(eq:ETSADAMStateSpace) is substituted by:
\begin{equation}
  \begin{aligned}
    {y}_{t} = &w(\hat{\mathbf{v}}_{t-\mathbf{l}}) + r(\hat{\mathbf{v}}_{t-\mathbf{l}}) e_t \\
    \hat{\mathbf{v}}_{t} = &f(\hat{\mathbf{v}}_{t-\mathbf{l}}) + \hat{g}(\hat{\mathbf{v}}_{t-\mathbf{l}}) e_t
  \end{aligned},
  (\#eq:ETSADAMStateSpaceEstimated)
\end{equation}
implying that the initial values of components and the smoothing parameters of the model are estimated. An example is the ETS(A,A,A) model applied to the data:
\begin{equation}
  \begin{aligned}
    y_{t} = & \hat{l}_{t-1} + \hat{b}_{t-1} + \hat{s}_{t-m} + e_t \\
    \hat{l}_t = & \hat{l}_{t-1} + \hat{b}_{t-1} + \hat{\alpha} e_t \\
    \hat{b}_t = & \hat{b}_{t-1} + \hat{\beta} e_t \\
    \hat{s}_t = & \hat{s}_{t-m} + \hat{\gamma} e_t 
  \end{aligned},
  (\#eq:ETSADAMAAAEstimated)
\end{equation}
where the initial values $\hat{l}_0, \hat{b}_0$ and $\hat{s}_{-m+2}, ... \hat{s}_0$ are estimated and then influence all the future values of components via the recursion \@ref(eq:ETSADAMAAAEstimated).

The estimation itself does not happen on its own, a complicated process of minimisation / maximisation of the pre-selected loss function by changing the values of parameters is involved. The results of this will differ depending on:

1. what distribution you assume,
2. what loss you use,
3. what the initial values of parameters that you feed to the optimiser are,
4. what the parameters of the optimiser are,
5. what the sample size is,
6. how many parameters you need to estimate and
7. what restriction you impose on parameters.

All of these aspects will be covered in this chapter.

Note that the discussions in this chapter are widely applicable to other dynamic models, such as ARIMA, which will be discussed later in this textbook.


## Maximum Likelihood Estimation (MLE) {#ADAMETSEstimationLikelihood}

The maximum likelihood estimation (MLE) of the ADAM ETS model relies on the distributional assumptions of each specific model and might differ from model to another. There are several options for the `distribution` supported by `adam()` function in smooth package, here we will briefly discuss how the estimation is done for each one of them. We start with the additive error models, for which the assumptions, log-likelihoods and MLE of scales are provided in Table \@ref(tab:additiveErrorLikelihoods). The likelihoods are derived based on the probability density functions, discussed in [the chapter 2](#distributions), by taking logarithms of the product of pdfs for all in sample observations. For example, this is what we get for the normal distribution:
\begin{equation}
  \begin{aligned}
    \mathcal{L}(\boldsymbol{\theta}, {\sigma}^2 | \mathbf{y}) = & \prod_{t=1}^T \left(\frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{\left(y_t - \mu_t \right)^2}{2 \sigma^2} \right)\right) \\
    \mathcal{L}(\boldsymbol{\theta}, {\sigma}^2 | \mathbf{y}) = & \frac{1}{\left(\sqrt{2 \pi \sigma^2}\right)^T} \exp \left( \sum_{t=1}^T -\frac{\epsilon_t^2}{2 \sigma^2} \right) \\
    \ell(\boldsymbol{\theta}, {\sigma}^2 | \mathbf{y}) = & \log \mathcal{L}(\boldsymbol{\theta}, {\sigma}^2 | \mathbf{y}) \\
    \ell(\boldsymbol{\theta}, {\sigma}^2 | \mathbf{y}) = & -\frac{T}{2} \log(2 \pi \sigma^2) -\frac{1}{2} \sum_{t=1}^T \frac{\epsilon_t^2}{\sigma^2}
  \end{aligned},
  (\#eq:ETSADAMNormalDistributionExample01)
\end{equation}
where $\mathbf{y}$ is the vector of all in-sample actual values. As for the scale, it is obtained by solving the equation after taking derivative of the log-likelihood \@ref(eq:ETSADAMNormalDistributionExample01) with respect to $\sigma^2$ and setting it equal to zero. We don't discuss the concentrated log-likelihoods (obtained after inserting the estimated scale in the respective log-likelihood function), because they are not useful in understanding how the model is estimated, but knowing how to calculate scale helps, because it simplifies the model estimation process.


```{r echo=FALSE}
# Assumption | log-likelihood \ell(\boldsymbol{\theta}, {\sigma}_\epsilon^2 | \mathbf{y}) | Scale
# Normal distribution
distributionsTable <- c("$\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$",
                        "$-\\frac{T}{2} \\log(2 \\pi \\sigma^2) -\\frac{1}{2} \\sum_{t=1}^T \\frac{\\epsilon_t^2}{\\sigma^2}$",
                        "$\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^T e_t^2$",
# Laplace distribution
                        "$\\epsilon_t \\sim \\mathcal{Laplace}(0, s)$",
                        "$-T \\log(2 s) -\\sum_{t=1}^T \\frac{|\\epsilon_t|}{s}$",
                        "$\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T |e_t|$",
# S distribution
                        "$\\epsilon_t \\sim \\mathcal{S}(0, s)$",
                        "$-T \\log(4 s^2) -\\sum_{t=1}^T \\frac{\\sqrt{|\\epsilon_t|}}{s}$",
                        "$\\hat{s} = \\frac{1}{2T} \\sum_{t=1}^T \\sqrt{|e_t|}$",
# GN distribution
                        "$\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)$",
                        "$\\begin{split} &T\\log\\beta -T \\log(2 s \\Gamma\\left(\\beta^{-1}\\right)) -\\\\
                                         &\\sum_{t=1}^T \\frac{\\left|\\epsilon_t\\right|^\\beta}{s}\\end{split}$",
                        "$\\hat{s} = \\sqrt[^{\\beta}]{\\frac{\\beta}{T} \\sum_{t=1}^T\\left| e_t \\right|^{\\beta}}$",
# Asymmetric Laplace distribution
                        "$\\epsilon_t \\sim \\mathcal{ALaplace}(0, s, \\alpha)$",
                        "$\\begin{split} &T\\log\\left(\\alpha(1-\\alpha)\\right) -T \\log(s) -\\\\
                                         &\\sum_{t=1}^T \\frac{\\epsilon_t (\\alpha - I(\\epsilon_t \\leq 0))}{s} \\end{split}$",
                        "$\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T e_t(\\alpha -I(e_t \\leq 0))$",
# IG distribution
                        "$1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\sim \\mathcal{IG}(1, s)$",
                        "$\\begin{split} &-\\frac{T}{2} \\log \\left(2 \\pi s \\left(\\frac{y_t}{\\mu_{y,t}}\\right)^3 \\right) -\\\\
                                         &\\frac{3}{2}\\sum_{t=1}^T \\log y_t -\\frac{1}{2s} \\sum_{t=1}^{T} \\frac{\\epsilon_t^2}{\\mu_{y,t}y_t}\\end{split}$",
                        "$\\hat{s} = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{e_t^2}{\\hat{\\mu}_{y,t} y_t}$",
# logN distribution
                        "$1+\\frac{\\epsilon_t}{\\mu_{y,t}} \\sim \\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)$",
                        "$\\begin{split} &-\\frac{T}{2} \\log \\left(2 \\pi \\sigma^2\\right) -\\\\
                                         &\\frac{1}{2\\sigma^2} \\sum_{t=1}^{T} \\left(\\log \\left(\\frac{y_t}{\\mu_{y,t}}\\right)+\\frac{\\sigma^2}{2}\\right)^2 -\\\\
                                         &\\sum_{t=1}^T \\log y_t \\end{split}$",
                        "$\\hat{\\sigma}^2 = 2\\left(1-\\sqrt{ 1-\\frac{1}{T} \\sum_{t=1}^{T} \\log^2\\left(\\frac{y_t}{\\hat{\\mu}_{y,t}}\\right)}\\right)$"
                        )
distributionsTable <- matrix(distributionsTable, 7, 3, byrow=TRUE,
                           dimnames=list(c("**Normal**","**Laplace**","**S**",
                                           "**Generalised Normal**","**Asymmetric Laplace**",
                                           "**Inverse Gaussian**","**Log Normal**"),
                                         c("**Assumption**","**log-likelihood**","**MLE of scale**")))
kableTable <- kableExtra::kable(distributionsTable, escape=FALSE, caption="Likelihood approach for additive error models",
                                col.names=c("Assumption","log-likelihood","MLE of scale"), label="additiveErrorLikelihoods")
kableExtra::kable_styling(kableTable, font_size=12, protect_latex=TRUE)
```

Other distributions can be used in ADAM ETS as well (for example, Logistic distribution or Student's t), but we do not discuss them here. Note that for the additive error models $y_t = \mu_{y,t}+\epsilon_t$, thus some formulae in Table \@ref(tab:additiveErrorLikelihoods) are simplified. In all of the cases in \@ref(tab:additiveErrorLikelihoods) the assumptions imply that the actual value follows the same distribution, but with a different location and / or scale. For example, for the Normal distribution we have:
\begin{equation}
  \begin{aligned}
    & \epsilon_t \sim \mathcal{N}(0, \sigma^2) \\
    & \mathrm{or} \\
    & y_t = \mu_{y,t}+\epsilon_t \sim \mathcal{N}(\mu_{y,t}, \sigma^2)
  \end{aligned}.
  (\#eq:ETSADAMNormalDistributionExample02)
\end{equation}

When it comes to the multiplicative error models, the likelihoods become slightly different. For example, when we assume that $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$ in the multiplicative error model, this implies that:
\begin{equation}
    y_t = \mu_{y,t}(1+\epsilon_t) \sim \mathcal{N}(\mu_{y,t}, \mu_{y,t}^2 \sigma^2) .
  (\#eq:ETSADAMNormalDistributionExample03)
\end{equation}
As a result the log-likelihoods would have the $\mu_{y,t}$ part in the formulae. Similar logic is applicable to $\mathcal{Laplace}$, $\mathcal{S}$, $\mathcal{GN}$ and $\mathcal{ALaplace}$ distributions. From the practical point of view, these assumptions imply that the scale (and variance) of the distribution of $y_t$ changes together with the level of the series. When it comes to the $\mathcal{IG}$ and log$\mathcal{N}$, the assumptions are imposed on the $1+\epsilon_t$ part of the model, the respective likelihoods do not involve the expectation $\mu_{y,t}$, but the formulation still implies that the variance of the data increases together with the increase of the level of data.

All the likelihoods for the multiplicative error models are summarised in Table \@ref(tab:multiplicativeErrorLikelihoods).

```{r echo=FALSE}
# Assumption | log-likelihood \ell(\boldsymbol{\theta}, {\sigma}_\epsilon^2 | \mathbf{y}) | Scale
# Normal distribution
distributionsTable <- c("$\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$",
                        "$\\begin{split} &-\\frac{T}{2} \\log(2 \\pi \\sigma^2) -\\frac{1}{2} \\sum_{t=1}^T \\frac{\\epsilon_t^2}{\\sigma^2} -\\\\
                                         &\\sum_{t=1}^T \\log |\\mu_{y,t}|\\end{split}$",
                        "$\\hat{\\sigma}^2 = \\frac{1}{T} \\sum_{t=1}^T e_t^2$",
# Laplace distribution
                        "$\\epsilon_t \\sim \\mathcal{Laplace}(0, s)$",
                        "$\\begin{split} &-T \\log(2 s) -\\sum_{t=1}^T \\frac{|\\epsilon_t|}{s} -\\\\
                                         &\\sum_{t=1}^T \\log \\mu_{y,t}\\end{split}$",
                        "$\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T |e_t|$",
# S distribution
                        "$\\epsilon_t \\sim \\mathcal{S}(0, s)$",
                        "$\\begin{split} &-T \\log(4 s^2) -\\sum_{t=1}^T \\frac{\\sqrt{|\\epsilon_t|}}{s} -\\\\
                                         &\\sum_{t=1}^T \\log \\mu_{y,t}\\end{split}$",
                        "$\\hat{s} = \\frac{1}{2T} \\sum_{t=1}^T \\sqrt{|e_t|}$",
# GN distribution
                        "$\\epsilon_t \\sim \\mathcal{GN}(0, s, \\beta)$",
                        "$\\begin{split} &T\\log\\beta -T \\log(2 s \\Gamma\\left(\\beta^{-1}\\right)) -\\\\
                                         &\\sum_{t=1}^T \\frac{\\left|\\epsilon_t\\right|^\\beta}{s} -\\sum_{t=1}^T \\log \\mu_{y,t}\\end{split}$",
                        "$\\hat{s} = \\sqrt[^{\\beta}]{\\frac{\\beta}{T} \\sum_{t=1}^T\\left| e_t \\right|^{\\beta}}$",
# Asymmetric Laplace distribution
                        "$\\epsilon_t \\sim \\mathcal{ALaplace}(0, s, \\alpha)$",
                        "$\\begin{split} &T\\log\\left(\\alpha(1-\\alpha)\\right) -T \\log(s) -\\\\
                                         &\\sum_{t=1}^T \\frac{\\epsilon_t (\\alpha - I(\\epsilon_t \\leq 0))}{s} -\\sum_{t=1}^T \\log \\mu_{y,t}\\end{split}$",
                        "$\\hat{s} = \\frac{1}{T} \\sum_{t=1}^T e_t(\\alpha -I(e_t \\leq 0))$",
# IG distribution
                        "$1+\\epsilon_t \\sim \\mathcal{IG}(1, s)$",
                        "$\\begin{split} &-\\frac{T}{2} \\log \\left(2 \\pi s \\left(1+\\epsilon_{t}\\right)^3 \\right) -\\\\
                                         &\\frac{3}{2}\\sum_{t=1}^T \\log y_t -\\frac{1}{2s} \\sum_{t=1}^{T} \\frac{\\epsilon_t^2}{1+\\epsilon_t}\\end{split}$",
                        "$\\hat{s} = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{e_t^2}{1+e_t}$",
# logN distribution
                        "$1+\\epsilon_t \\sim \\mathrm{log}\\mathcal{N}\\left(-\\frac{\\sigma^2}{2}, \\sigma^2\\right)$",
                        "$\\begin{split} &-\\frac{T}{2} \\log \\left(2 \\pi \\sigma^2\\right) -\\\\
                                         &\\frac{1}{2\\sigma^2} \\sum_{t=1}^{T} \\left(\\log \\left(1+\\epsilon_t\\right)+\\frac{\\sigma^2}{2}\\right)^2 -\\\\
                                         &\\sum_{t=1}^T \\log y_t \\end{split}$",
                        "$\\hat{\\sigma}^2 = 2\\left(1-\\sqrt{ 1-\\frac{1}{T} \\sum_{t=1}^{T} \\log^2\\left(1+e_t\\right)}\\right)$"
                        )
distributionsTable <- matrix(distributionsTable, 7, 3, byrow=TRUE,
                           dimnames=list(c("**Normal**","**Laplace**","**S**",
                                           "**Generalised Normal**","**Asymmetric Laplace**",
                                           "**Inverse Gaussian**","**Log Normal**"),
                                         c("**Assumption**","**log-likelihood**","**MLE of scale**")))
kableTable <- kableExtra::kable(distributionsTable, escape=FALSE, caption="Likelihood approach for multiplicative error models",
                                col.names=c("Assumption","log-likelihood","MLE of scale"), label="multiplicativeErrorLikelihoods")
kableExtra::kable_styling(kableTable, font_size=12, protect_latex=TRUE)
```

When it comes to practicalities, in `adam()` function, in the optimiser, on each iteration, after fitting the model the scales from Tables \@ref(tab:additiveErrorLikelihoods) and \@ref(tab:multiplicativeErrorLikelihoods) are calculated and then used in the log-likelihoods based on the respective distribution functions, for additive error models:

1. $\mathcal{N}$ - `dnorm(x=actuals, mean=fitted, sd=scale,  log=TRUE)` from `stats` package;
2. $\mathcal{Laplace}$ - `dlaplace(q=actuals, mu=fitted, scale=scale, log=TRUE)` from `greybox` package;
3. $\mathcal{S}$ - `ds(q=actuals, mu=fitted, scale=scale, log=TRUE)` from `greybox` package;
4. $\mathcal{GN}$ - `dgnorm(x=actuals, mu=fitted, alpha=scale, beta=beta, log=TRUE)` implemented internally based on `gnorm` package (the version on CRAN is outdated);
5. $\mathcal{ALaplace}$ - `dalaplace(q=actuals, mu=fitted, scale=scale,  alpha=alpha, log=TRUE)` from `greybox` package;
6. $\mathcal{IG}$ - `dinvgauss(x=actuals, mean=fitted, dispersion=scale/fitted, log=TRUE)` from `statmod` package;
7. log$\mathcal{N}$ - `dlnorm(x=actuals, meanlog=fitted-scale^2/2, sdlog=scale, log=TRUE)` from `stats` package;

and for multiplicative error models:

1. $\mathcal{N}$ - `dnorm(x=actuals, mean=fitted, sd=scale*fitted,  log=TRUE)`;
2. $\mathcal{Laplace}$ - `dlaplace(q=actuals, mu=fitted, scale=scale*fitted, log=TRUE)`;
3. $\mathcal{S}$ - `ds(q=actuals, mu=fitted, scale=scale*sqrt(fitted), log=TRUE)`;
4. $\mathcal{GN}$ - `dgnorm(x=actuals, mu=fitted, alpha=scale*fitted^beta, beta=beta, log=TRUE)`;
5. $\mathcal{ALaplace}$ - `dalaplace(q=actuals, mu=fitted, scale=scale*fitted,  alpha=alpha, log=TRUE)`;
6. $\mathcal{IG}$ - `dinvgauss(x=actuals, mean=fitted, dispersion=scale/fitted, log=TRUE)`;
7. log$\mathcal{N}$ - `dlnorm(x=actuals, meanlog=fitted-scale^2/2, sdlog=scale, log=TRUE)`.

Note that in cases of $\mathcal{GN}$ and $\mathcal{ALaplace}$, additional parameters (namely $\beta$ and $\alpha$) are needed. If the user does not provide them, then they are estimated together with the other parameters via the maximisation of respective likelihoods.

The MLE of parameters of ADAM ETS models makes them comparable with each other irrespective of the types of components and distributional assumptions. As a result, model selection based on information criteria can be done using `auto.adam()` function from `smooth`, which will select the most appropriate ETS model with the most suitable distribution.


### An example in R
`adam()` function in smooth package has the parameter `distribution`, which allows selecting between several options discussed in this chapter, based on the respective density functions (see the list above with `dnorm()` etc). Here is a brief example in R with ADAM ETS(M,A,M) applied to the `AirPassengers` data with several distributions:
```{r}
adamModel <- vector("list",4)
adamModel[[1]] <- adam(AirPassengers, "MAM", distribution="dnorm", h=12, holdout=TRUE)
adamModel[[2]] <- adam(AirPassengers, "MAM", distribution="dlaplace", h=12, holdout=TRUE)
adamModel[[3]] <- adam(AirPassengers, "MAM", distribution="dgnorm", h=12, holdout=TRUE)
adamModel[[4]] <- adam(AirPassengers, "MAM", distribution="dinvgauss", h=12, holdout=TRUE)
```

In this case, the function will select the most appropriate ETS model for each distribution. We can see what was selected in each case and compare the models using information criteria:

```{r}
data.frame(AICc=sapply(adamModel, AICc), row.names=c("dnorm","dlaplace","dgnorm","dinvgauss"))
```
We could compare the performance of models in detail, but, for the purposes of this demonstration, it should suffice to say that among the four models considered above, based on the AICc value, the model with the Normal distribution should be preferred. This process of fit and selection can be automated using `auto.adam()` model, which accepts the vector of distributions to test and by default would consider `distribution=c("dnorm", "dlaplace", "ds", "dgnorm", "dlnorm", "dinvgauss")`:
```{r eval=FALSE}
adamModel <- auto.adam(AirPassengers, "MAM", h=12, holdout=TRUE)
```
This command should return the ADAM ETS(M,A,M) model with the most appropriate distribution, selected based on the AICc.


## Non MLE-based loss functions
### MSE and MAE
An alternative approach for estimating ADAM ETS is using the conventional loss functions, such as MSE, MAE etc. In this case, the model selection using information criteria would not work, but this might not matter, when you have already decided what model to use and want to improve it. In some special cases (as [discussed earlier](#distributions)), the minimisation of these losses would give the same results as the maximisation of some likelihoods, namely:

1. Minimum of MSE corresponds to the maximum of likelihood of Normal distribution;
2. Minimum of MAE corresponds to the maximum of likelihood of Laplace distribution;
3. Minimum of pinball function [@Koenker1978, quantile regression] corresponds to the maximum of likelihood of Asymmetric Laplace distribution [@Yu2005];

The main difference between using these losses and maximising respective likelihoods is in the [number of estimated parameters](#statisticsNumberOfParameters): the latter implies that the scale is estimated together with the other parameters, while the former does not consider it.

Having said that, the assumed distribution does not necessarily depend on the used loss and vice versa. For example, we can assume that the actuals follow Inverse Gaussian distribution, but estimate the model using MAE. The estimates of parameters might not be as efficient as in the case of MLE, but it is still possible to do. The error term, which is minimised in respective losses depends on the error type:

- Additive error: $e_t = y_t - \hat{\mu}_{y,t}$;
- Multiplicative error: $e_t = \frac{y_t - \hat{\mu}_{y,t}}{\hat{\mu}_{y,t}}$.

This follows directly from the respective ETS models.

### HAM
Along with the discussed MSE and MAE, there is also HAM - "Half Absolute Moment":
\begin{equation}
  \mathrm{HAM} = \frac{1}{T} \sum_{j=1}^T \sqrt{\left|e_t\right|},
  (\#eq:HAM)
\end{equation}
the minimum of which corresponds to the maximum of the likelihood of [S distribution](#distributions). The idea of this estimator is to minimise the errors that happen very often, close to the estimate. It will typically ignore the outliers and focus on the values that happen most often. As a result, if used for the integer values, the minimum of HAM would correspond to the mode of that distribution. I donot have a proof of this property, but it becomes apparent, given that the square root in \@ref(eq:HAM) would reduce the influence of all values lying above 1 and increase the values of everything that lies between (0, 1) (e.g. $\sqrt{0.16}=0.4$, but $\sqrt{16}=4$).

Similar to HAM, one can calculate other fractional losses, which would be even less sensitive to outliers and more focused on the frequently appearing values, e.g. by using the $\sqrt[^\alpha]{\left|e_t\right|}$ with $\alpha>1$.

### LASSO and RIDGE
While this is not a well studied approach yet, it is possible to use LASSO [@Tibshirani1996] and RIDGE for the estimation of ETS models [@James2017 give a good overview of these losses with examples in R], which can be formulated as (at least this is how it is formualted in ADAM ETS):
\begin{equation}
  \begin{aligned}
    \mathrm{LASSO} = &(1-\lambda) \frac{1}{T} \sum_{j=1}^T e_t^2 + \lambda \sum |\hat{\theta}| \\
    \mathrm{RIDGE} = &(1-\lambda) \frac{1}{T} \sum_{j=1}^T e_t^2 + \lambda \sum \hat{\theta}^2,
  \end{aligned}
  (\#eq:Regularisation)
\end{equation}
where $\theta$ is the vector of all parameters in the model and $\lambda$ is the regularisation parameter. The idea of these losses is in shrinkage of parameters. If $\lambda=0$, then the losses become equivalent to the MSE, when $\lambda=1$, the optimiser would minimise the values of parameters, ignoring the MSE part. In the context of ETS, it makes sense to shrink everything but the initial level. In order for different components to shrink with a similar speed, they need to be normalised, so here are some tricks used in ADAM ETS:

1. The smoothing parameters are left intact, because they typicall lie between 0 and 1, where 0 means that the respective states are not updated over time;
2. The initial level is normalised using the formula: $\hat{l}_0' = \frac{\hat{l}_0 - \bar{y}_m}{\hat{\sigma}_{y,m}}$, where $\bar{y}_m$ is the mean and $\hat{\sigma}_{y,m}$ is the standard deviation of the first $m$ actual observations, where $m$ is the highest frequency of the data (if $m=1$, then both values are taken based on the first two observations). Shrinking it to zero implies that it becomes equivalent to the mean of the first $m$ observations;
3. If the trend is *additive*, then the initial trend component is scaled using: $\hat{b}_0' = \frac{\hat{b}_0}{\hat{\sigma}_{y,m}}$, making sure that it shrinks towards zero (no trend). In case of the *multiplicative* trend, this becomes: $\hat{b}_0' = \log{\hat{b}_0}$ , making it shrink towards 1 in the original scale (again, no trend case);
4. If the seasonal component is *additive*, then the normalisation similar to the one in trend is done for every seasonal index: $\hat{s}_i' = \frac{\hat{s}_i}{\hat{\sigma}_{y,m}}$, making sure that they shrink towards zero. If the seasonal component is *multiplicative*, then the logarithm of components is taken: $\hat{s}_i' = \log{\hat{s}_i}$, making sure that they shrink towards 1 in the original scale.
5. If there are explanatory variables and the error term of the model is *additive*, then the respective parameters are divided by the standard deviations of the respective variables. In case of *multiplicative* error term, nothing is done, because the parameters in this case would typically be close to zero anyway (see a chapter on the ADAM ETSX).

All of these tricks make sure that different components shrink towards zero, not breaking the model. As a result, one can potentially use the model with trend and seasonality and use regularisation in order to shrink the unnecessary parameters towards zero. The `adam()` function does not select the most appropriate $\lambda$ and will set it equal to zero if it is not provided by the user.

Note that both LASSO and RIDGE are experimental options. If you have better ideas of how to implement them, send me a message, I will improve the mechanism in `adam()`.

### Custom losses
It is also theoretically possible to use other non-standard loss functions. `adam()` function allows doing that via the parameter `loss`. For example, we could estimate an ETS(A,A,N) model on the `BJsales` data using an absolute cubic loss (note that the parameters `actual`, `fitted` and `B` are compulsory for the function):
```{r eval=FALSE}
lossFunction <- function(actual, fitted, B){
  return(mean(abs(actual-fitted)^3));
}
adam(BJsales, "AAN", loss=lossFunction, h=10, holdout=TRUE)
```

where `actual` is the vector of actual values $y_t$, `fitted` is the estimate of the one step ahead conditional mean $\hat{\mu}_{y,t}$ and $B$ is the vector of all estimated parameters, $\hat{\theta}$. This way, one can use more advanced estimators, such as, for example, M-estimators [@Barrow2020].

### Examples in R
`adam()` has two parameters, one regulating the assumed `distribution`, and another one, regulating how the model will be estimated, what `loss` will be used for these purposes. Here are examples with combinations of different losses and an assumed Inverse Gaussian distribution for ETS(M,A,M) on `AirPassengers` data. We start with MSE, MAE and HAM:
```{r}
adamModel <- vector("list",6)
names(adamModel) <- c("likelihood","MSE", "MAE", "HAM", "LASSO", "Huber")
adamModel[[1]] <- adam(AirPassengers, "MAM", distribution="dinvgauss",
                       h=12, holdout=TRUE, loss="likelihood")
adamModel[[2]] <- adam(AirPassengers, "MAM", distribution="dinvgauss",
                       h=12, holdout=TRUE, loss="MSE")
adamModel[[3]] <- adam(AirPassengers, "MAM", distribution="dinvgauss",
                       h=12, holdout=TRUE, loss="MAE")
adamModel[[4]] <- adam(AirPassengers, "MAM", distribution="dinvgauss",
                       h=12, holdout=TRUE, loss="HAM")
```

In these three cases the models, assuming the same distribution for the error term are estimated using MSE, MAE and HAM. Their smoothing parameters should differ, with MSE producing fitted values closer to the mean, MAE - closer to the median and HAM - closer to the mode (but not exactly the mode) of the distribution.

In addition, we introduce ADAM ETS(M,A,M) estimated using LASSO with arbitrarily selected $\lambda=0.1$:
```{r}
adamModel[[5]] <- adam(AirPassengers, "MAM", distribution="dinvgauss",
                       h=12, holdout=TRUE, loss="LASSO", lambda=0.1)
```
And, finally, we estimate the same model using a custom loss, which in this case is Huber loss with the threshold of 1.345:
```{r}
# Huber loss with a threshold of 1.345
lossFunction <- function(actual, fitted, B){
  errors <- actual-fitted;
  return(sum(errors[errors<=1.345]^2) + sum(abs(errors)[errors>1.345]));
}
adamModel[[6]] <- adam(AirPassengers, "MAM", distribution="dinvgauss",
                       h=12, holdout=TRUE, loss=lossFunction)
```

Now we can compare the performance of the six models. First, we can compare the smoothing parameters:

```{r}
round(sapply(adamModel,"[[","persistence"),3)
```

What we sould observe in this case is that LASSO should have the lowest smoothing parameters, because they are shrunk directly in the model. Likelihood and MSE should probably give similar values, because they both rely on squared errors, but not the same because the likelihood of Inverse Gaussian also has additional elements in it.

Unfortunately, we do not have information criteria for the models 2 - 6 in this case, because the likelihood function is not maximised with these losses, so it's not possible to compare them via the in-sample statistics. But we can compare their holdout sample performance:

```{r}
round(sapply(adamModel,"[[","accuracy"),3)[c("ME","MAE","MSE"),]
```

And we can also produce forecasts from them and plot those forecasts for the visual inspection:
```{r fig.width=8, fig.height=10}
adamModelForecasts <- lapply(adamModel,forecast, h=12, interval="prediction")
layout(matrix(c(1:6),3,2,byrow=TRUE))
for(i in 1:6){
  plot(adamModelForecasts[[i]],
       main=paste0("ETS(MAM) estimated using ",names(adamModel)[i]))
}
```

What we observe in this case, is that different losses led to different forecasts and prediction intervals (we still assume Inverse Gaussian distribution) and that in case of LASSO, the shrinkage is so strong that the seasonality is shrunk to zero. What can potentially be done to make this practical is the [rolling origin evaluation](#rollingOrigin) for different losses and then comparison of forecast errors between them in order to select the most efficient one.


## Multistep losses
Another class of losses that can be used in the estimation of ADAM models is the multistep losses. The idea behind them is to produce the point forecast for $h$ steps ahead from each observation in sample and then calculate a measure based on that, which will be minimised by the optimiser in order to find the most suitable values of parameters. There is a lot of literature on this topic, @Svetunkov2020Multistep studied them in detail, showing that their usage implies shrinkage of smoothing parameters in case of ETS models. In this section we will discuss the most popular multistep losses, see what they imply and make a connection between these losses and predictive likelihoods from the ETS models.

One of the simplest estimators is the $\mathrm{MSE}_h$ - mean squared $h$-steps ahead error:
\begin{equation}
	\mathrm{MSE}_h = \frac{1}{T-h} \sum_{t=1}^{T-h} e_{t+h|t}^2 ,
  (\#eq:hstepsMSEPopulation)
\end{equation}
where $e_{t+h|t} = y_{t+h} - \hat{\mu}_{t+h|t}$ is the conditional $h$ steps ahead forecast error on the observation $t+h$ produced from the point at time $t$. This estimator is sometimes used to fit a model several times, for each horizon from 1 to $h$ steps ahead, resulting in $h$ different values of parameters for each $j=1, \ldots, h$. The estimation process in this case becomes at least $h$ times more complicated than estimating one model, but is reported to result in increased accuracy. @Svetunkov2020Multistep show that MSE$_h$ is proportional to the h steps ahead forecast error variance $V(y_{t+h}|t)=\sigma^2_h$, which implies that the minimisation of \@ref(eq:hstepsMSEPopulation) leads to the minimisation of the variance and in turn to the minimisation of both one step ahead MSE and a combination of smoothing parameters of a model. This becomes more obvious in the case of [pure additive ETS](#ADAMETSPureAdditive), where the [analytical formulae for variance](#pureAdditiveExpectationAndVariance) are available. The parameters are shrunk towards zero in case of ETS, making the model deterministic. In case of ARIMA, the mechanism is similar, making models closer to the deterministic ones, but the direction of shrinkage is more complicating. The strength of shrinkage is proportional to the forecast horizon $h$ and is weakened with the increase of the sample size.

@Svetunkov2020Multistep demonstrate that the minimum of MSE$_h$ corresponds to the maximum of the predictive likelihood based on the normal distribution, assuming that $\epsilon_t \sim N(0,\sigma^2)$. The log-likelihood in this case is:
\begin{equation}
	\ell_{\mathrm{MSE}_h}(\theta, {\sigma^2_h} | \mathbf{y}) = -\frac{T-h}{2} \left( \log(2 \pi) + \log \sigma^2_h \right) -\frac{1}{2} \sum_{t=1}^{T-h} \left( \frac{\eta_{t+h}^2}{\sigma^2_h} \right) ,
  (\#eq:LikelihoodMSEh)
\end{equation}
where $\eta_{t+h} \sim N(0, \sigma_h^2)$ is the h steps ahead forecast error, conditional on the information available at time $t$, $\theta$ is the vector of all estimated parameters of the model and $\mathbf{y}$ is the vector of $y_{t+h}$ for all $t=1,..,T-h$. The formula \@ref(eq:LikelihoodMSEh) can be used for the calculation of information criteria and in turn for the model selection in cases, when MSE$_h$ is used for the model estimation. 

