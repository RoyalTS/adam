# Explanatory variables in ADAM {#ETSX}
Having the state space model \@ref(eq:ETSADAMStateSpace) allows easily extending the model to include additional components and explanatory variables. Furthremore, parameters for these additional components can either be fixed or change over time. The model becomes more complicated in the latter case and more difficult to estimate, but nonetheless still potentially useful.

In practice, the need for explanatory variables arises, when there are some external factors influencing the response variable, which cannot be ignored and impact the final forecasts. Examples of such variables in demand forecasting context include price changes, promotional activities, temperature etc. In some cases the changes in these factors would not have a substantial impact on the demand, but in the others they would be essential for improving the accuracy.

While inclusion of explanatory variables in context of ARIMA models is relatively well studied idea, in case of ETS, there is only a handful of papers on the topic. One of such papers is @Koehler2012, which discusses the mechanism of outlier detection and approximation of outliers via an ETSX model (ETS with explanatory variables). The authors show that if an outlier appears at the end of series, then it will have a serious impact on the final forecast. However, if it appears either in the middle or in the beginning of series, the impact on the final forecast is typically negligible. This is relevant to our discussion, because there is a direct link between dealing with outlier in @Koehler2012 and including explanatory variables in ETSX.

In this chapter we discuss the main aspects of ETSX model, how it is formulated in ADAM framework and how the more advanced models can be built upon it.


## ETSX: Model formulation
As discussed previously, there are fundamentally two types of ETS models:

1. Additive error model [which was discussed in @Hyndman2008b in Chapter 9],
2. Multiplicative error model.

The inclusion of explanatory variables in ADAM ETSX is determined by the type of the error, so that in case of (1) the measurement equation of the model is:
\begin{equation}
  {y}_{t} = a_{0,t} + a_{1,t} x_{1,t} + a_{2,t} x_{2,t} + \dots + a_{n,t} x_{n,t} + \epsilon_t ,
  (\#eq:ETSXADAMStateSpacePureAdditiveMeasurement)
\end{equation}
where $a_{0,t}$ is the point value based on all ETS components (for example, $a_{0,t}=l_{t-1}$ in case of ETS(A,N,N)), $x_{i,t}$ is the $j$-th explanatory variable, $a_{i,t}$ is the parameter for that component and $n$ is the number of explanatory variables. We will call the estimated parameters of such model $\hat{a}_{i,t}$. In the simple case, the transition equation for such model would imply that the parameters $a_{i,t}$ do not change over time:
\begin{equation}
  \begin{aligned}
    &a_{1,t} = a_{1,t-1} \\
    &a_{2,t} = a_{2,t-1} \\
    &\vdots \\
    &a_{n,t} = a_{n,t-1}
  \end{aligned} .
  (\#eq:ETSXADAMStateSpacePureAdditiveTransition)
\end{equation}
Complex mechanisms for the states update can be proposed instead of \@ref(eq:ETSXADAMStateSpacePureAdditiveTransition), but we do not discuss them at this point. Typically, the initial values of parameters would be estimated at the optimisation stage, either based on likelihood or some other loss function, so the index $t$ can be dropped, substituting $a_{1,t}=a_{1}$.

When it comes to the mulitplicative error, the multiplication should be used instead of addition. However, it is easier to formulate the model in logarithms in order to linearise it:
\begin{equation}
  \log {y}_{t} = \log a_{0,t} + a_{1,t} x_{1,t} + a_{2,t} x_{2,t} + \dots + a_{n,t} x_{n,t} + \log(1+ \epsilon_t).
  (\#eq:ETSXADAMStateSpacePureAdditiveMeasurement)
\end{equation}
Note that if log-log model is required, all that needs to be done, is that $x_{i,t}$ should be substituted by $\log x_{j,t}$.

One of the other ways to formulate the ETSX model is to move the explanatory variables $x_{i,t}$ in the measurement vector $\mathbf{w}_{t}$, making it change over time, to move the parameters in the state vector, add diagonal matrix to the existing transition matrix and set values of the persistence vector for the parameters of explanatory variables to zero. The [general state space model](#ADAMETSIntroduction) does not change in that case, but the pure ones can be specifically represented as:
\begin{equation}
  \begin{aligned}
    {y}_{t} = & \mathbf{w}'_t \mathbf{v}_{t-\mathbf{l}} + \epsilon_t \\
    \mathbf{v}_t = & \mathbf{F} \mathbf{v}_{t-\mathbf{l}} + \mathbf{g} \epsilon_t
  \end{aligned} 
  (\#eq:ETSXADAMStateSpacePureAdditiveFull)
\end{equation}
and 
\begin{equation}
  \begin{aligned}
    {y}_{t} = & \exp\left(\mathbf{w}'_t \log \mathbf{v}_{t-\mathbf{l}} + \log(1 + \epsilon_t)\right) \\
    \log \mathbf{v}_t = & \mathbf{F} \log \mathbf{v}_{t-\mathbf{l}} + \log(\mathbf{1}_k + \mathbf{g} \epsilon_t)
  \end{aligned}. 
  (\#eq:ETSXADAMStateSpacePureAdditiveFull)
\end{equation}
So, the only thing that changes in these models is the time varying measurement vector $\mathbf{w}'_t$ instead of the fixed one. For example, in case of ETSX(A,A,A) we will have:
\begin{equation}
  \begin{aligned}
    \mathbf{F} =
    \begin{pmatrix} 1 & 1 & 0 & 0 & \dots & 0 \\
                    0 & 1 & 0 & 0 & \dots & 0 \\
                    0 & 0 & 1 & 0 & \dots & 0 \\
                    0 & 0 & 0 & 1 & \dots & 0 \\
                    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & 0 & 0 & \dots & 1
    \end{pmatrix}
    & \mathbf{w}_t = \begin{pmatrix} 1 \\ 1 \\ 1 \\ x_{1,t} \\ \vdots \\x_{n,t} \end{pmatrix}
    & \mathbf{g} = \begin{pmatrix} \alpha \\ \beta \\ \gamma \\ 0 \\ \vdots \\ 0 \end{pmatrix} \\
    & \mathbf{v}_{t} = \begin{pmatrix} l_t \\ b_t \\ s_t \\ a_{1,t} \\ \vdots \\ a_{n,t} \end{pmatrix}
    & \mathbf{l} = \begin{pmatrix} 1 \\ 1 \\ m \\ 1 \\ \vdots \\ 1 \end{pmatrix}
  \end{aligned},
  (\#eq:ETSXADAMAAAMatrices)
\end{equation}
which is equivalent to the combination of equations \@ref(eq:ETSXADAMStateSpacePureAdditiveMeasurement) and \@ref(eq:ETSXADAMStateSpacePureAdditiveTransition), giving us:
\begin{equation}
  \begin{aligned}
    y_{t} = & l_{t-1} + b_{t-1} + s_{t-m} + a_{1,t} x_{1,t} + \dots + a_{n,t} x_{n,t} + \epsilon_t \\
    l_t = & l_{t-1} + b_{t-1} + \alpha \epsilon_t \\
    b_t = & b_{t-1} + \beta \epsilon_t \\
    s_t = & s_{t-m} + \gamma \epsilon_t \\
    a_{1,t} = &a_{1,t-1} \\
    \vdots &\\
    a_{n,t} = &a_{n,t-1}
  \end{aligned}.
  (\#eq:ETSXADAMAAA)
\end{equation}
Alternatively, the state, measurement and persistence vectors and transition matrix can be split into two, separatign the ETS and X parts in the state space equations.

When all the smoothing parameters of the ETS part of the model are equal to zero, the ETSX reverts to a deterministic model, directly related to the multiple linear regression. For example, in case of ETSX(A,N,N) with $\alpha=0$ we get:
\begin{equation}
  \begin{aligned}
    y_{t} = & l_{t-1} + a_{1,t} x_{1,t} + \dots + a_{n,t} x_{n,t} + \epsilon_t \\
    l_t = & l_{t-1} \\
    a_{1,t} = & a_{1,t-1} \\
    \vdots & \\
    a_{n,t} = & a_{n,t-1}
  \end{aligned},
  (\#eq:ETSXADAMANNDeterministic)
\end{equation}
where $l_t=a_0$ is the intercept of the model. \@ref(eq:ETSXADAMANNDeterministic) can be rewritten in the conventional way, dropping the transition part of the state space model:
\begin{equation}
    y_{t} = a_0 + a_{1} x_{1,t} + \dots + a_{n} x_{n,t} + \epsilon_t .
  (\#eq:linearRegression)
\end{equation}
So, in general ETSX implies that we are dealing with regression with time varying intercept, where the principles of this variability are defined by the ETS components and smoothing parameters (e.g. intercept can vary seasonally). Similar properties are obtained with the multiplicative error model, with the main difference that the impact of explanatory variables on the response variable will vary with the changes of the intercept.


## Conditional expectation and variance of ETSX
### The conventional ETSX
ETS models have a serious limitation, which we will discuss in one of the latter chapters: they assume that the parameters of the model are known, i.e. there is no variability in them and that the in-sample values are fixed no matter what. This limitation also impacts the ETSX part. While in case of point forecasts this is not an issue, this impacts the conditional variance and prediction intervals. As a result, the conditional mean and variance of the conventional ETSX assume that the parameters $a_0, \dots a_n$ are also known, leading to the following formulae in case of pure additive model, based on what was discussed in [the section on pure additive models](#pureAdditiveExpectationAndVariance):
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = & \sum_{i=1}^d \left(\mathbf{w}_{m_i,t}' \mathbf{F}_{m_i}^{\lceil\frac{h}{m_i}\rceil-1} \right) \mathbf{v}_{t} \\
    \text{V}(y_{t+h}|t) = & \left( \sum_{i=1}^d \left(\mathbf{w}_{m_i,t}' \sum_{j=1}^{\lceil\frac{h}{m_i}\rceil-1} \mathbf{F}_{m_i}^{j-1} \mathbf{g}_{m_i} \mathbf{g}'_{m_i} (\mathbf{F}_{m_i}')^{j-1} \mathbf{w}_{m_i,t} \right) + 1 \right) \sigma^2
  \end{aligned},
  (\#eq:ETSXADAMStateSpaceANNRecursionMeanAndVarianceGeneral)
\end{equation}
the main difference from [the conventional model](#pureAdditiveExpectationAndVariance) being is the index $t$ in the measurement vector. As an example, here how the two statistics will look in case of ETSX(A,N,N):
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = & l_{t} + \sum_{i=1}^n a_i x_{i,t+h} \\
    \text{V}(y_{t+h}|t) = & \left((h-1) \alpha^2 + 1 \right) \sigma^2
  \end{aligned},
  (\#eq:ETSXADAMStateSpaceANNRecursionMeanAndVariance)
\end{equation}
where the variance ignores the potential variability rising from the explanatory variables because of the ETS limitations. As a result, the prediction and confidence intervals for the ETSX model would typically be narrower than needed and would only be adequate in cases of large samples, where [law of large numbers](#LLNandCLT) would start working, reducing the variance of parameters (this is assuming that the [typical assumptions](#assumptions) of the model hold).


### ETSX with random explanatory variables
Note that the ETSX works well in cases, when the future values of $x_{i,t+h}$ are known, which is not always the case. It is a realistic assumption, when we have control over the explanatory variables (e.g. prices and promotions for our product). But in the case, when the variables are out of our control, they need to be forecasted somehow. In this case we are assuming that each $x_{i,t}$ is a random variable with some dynamic conditional one step ahead expectation $\mu_{x_{i,t}}$ and a one step ahead variance $\sigma^2_{x_{i,1}}$. This will change the conditional moments of the model. Here what we will have in case of ETSX(A,N,N) (given that the [typical assumptions](#assumptions) hold):
\begin{equation}
  \begin{aligned}
    \mu_{y,t+h} = \text{E}(y_{t+h}|t) = & l_{t} + \sum_{i=1}^n a_i \mu_{x_{i,t+h}} \\
    \text{V}(y_{t+h}|t) = & \left((h-1) \alpha^2 + 1 \right) \sigma^2 + \sum_{i=1}^n a^2_i \sigma^2_{x_{i,h}} + 2 \sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i a_j \sigma_{x_{i,h},x_{j,h}}
  \end{aligned},
  (\#eq:ETSXADAMStateSpaceANNRecursionMeanAndVarianceRandomness)
\end{equation}
where $\sigma^2_{x_{i,h}}$ is the variance of $x_{i}$ h steps ahead, $\sigma_{x_{i,h},x_{j,h}}$ is the h steps ahead covariance between the explanatory variables $x_{i,h}$ and $x_{j,h}$, both conditional on the information available at the observation $t$. similarly, if we are interested in one step ahead point forecast from the model, it should take the randomness of explanatory variables into account and become:
\begin{equation}
  \begin{aligned}
    \mu_{y,t} = &  \left. \mathrm{E}\left(l_{t-1} + \sum_{i=1}^n a_i x_{i,t} + \epsilon_{t} \right| t-1 \right) = \\
                = & l_{t-1} + \sum_{i=1}^n a_i \mu_{x_{i,t}}
  \end{aligned}.
  (\#eq:ADAMETSXANNStepAhead)
\end{equation}
So, in case of ETSX with random explanatory variables, the model should be constructed based on the expectations of those variables, not the random values themselves. This does not appear in the context of the classical linear regression, because it does not rely on the one step ahead recursion. But this explains, for example, why @Athanasopoulos2011 found that some models with predicted explanatory variables works better than the model with the variables themselves. This becomes important, when estimating the model, such as ETS(A,N,N), when the following is constructed:
\begin{equation}
  \begin{aligned}
    \hat{y}_{t} = & \hat{l}_{t-1} + \sum_{i=1}^n \hat{a}_{i,t} \hat{x}_{i,t} \\
    e_t = & y_t - \hat{y}_{t} \\
    \hat{l}_{t} = & \hat{l}_{t-1} + \hat{\alpha} e_t \\
    \hat{a}_{i,t} = & \hat{a}_{i,t-1} \text{ for each } i \in \{1, \dots n\}
  \end{aligned},
  (\#eq:ADAMETSXANNStepAhead)
\end{equation}
where $\hat{x}_{i,t}$ is the in-sample conditional one step ahead mean for the explanatory variable $x_i$.

Summarising this section, the adequate ETSX model needs to be able to work in at least two regimes: (1) assuming that the explanatory variable is known, (2) assuming that the explanatory variable is random.

Finally, as discussed previously, the conditional moments for the pure and mixed models do not have closed forms in general, implying that the simulations need to be carried out.


## Stability and forecastability conditions of ETSX
It can be shown that any ETSX is not stable in terms of ETS models, meaning that the weights of such model do not decline exponentially to zero. This becomes apparent, when we compare the explanatory part of any ETSX with a deterministic model, discussed in the context of [pure additive models](#stabilityConditionAdditiveError). For example, we have already discussed that when $\alpha=0$ in ETS(A,N,N), then the model looses the stability condition, but it still can be forecastable. In fact the relation between ETSX and regression makes shows that the stability / forecastability conditions should be calculated for the ETS part, ignoring the X, as long as the parameters of the regression model do not vary over time.


