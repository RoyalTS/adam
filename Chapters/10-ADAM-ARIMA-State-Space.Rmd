# ADAM ARIMA {#ADAMARIMA}
There are different ways to formulate and implement ARIMA. The one discussed in the [previous section](#ARIMA) is the conventional model, which can be estimate directly, assuming that the initialisation of the model happens some time before the Big Bang: the conventional ARIMA assumes that there is no starting point of the model, we just observe a specific piece of data from a population without any beginning or end. Obviously this assumption is idealistic and does not necessarily agree with reality (imagine the series of infinitely lasting sales of Siemens mobile phones).

Besides the conventional formulation, there is also a state space form of ARIMA, implemented in SSOE [@Hyndman2008b]. @Svetunkov2019 adapted this state space model for supply chain forecasting, developing an order selection mechanism, sidesteping the hypothesis testing and focusing on information criteria. However, the main issue with this approach is that the resulting ARIMA model works very slow on the data with large frequencies (because the transition matrix becomes huge). Luckily, there is an alternative SSOE state space formulation, using the same idea of lags as [ADAM ETS](#ADAMETSIntroduction). This model is already implemented in `msarima()` function of `smooth` package and was also used as the basis for the ADAM ARIMA.


## State space ARIMA
### Model formulation
In order to develop state space ARIMA, we will use the most general multiple seasonal ARIMA, discussed in [the previous section](#MSARIMA):
\begin{equation*}
  y_t \prod_{j=0}^n \Delta^{D_j} (B^{m_j}) \varphi^{P_j}(B^{m_j}) = \epsilon_t \prod_{j=0}^n \vartheta^{Q_j}(B^{m_j}) ,
\end{equation*}
This model can be represented in an easier to digest form by expanding the polynomials on the left hand side of the equation and moving all the previous values to the right hand side and then expanding the MA polynomials:
\begin{equation}
  y_t = \sum_{j=1}^K \eta_j y_{t-j} + \sum_{j=1}^K \theta_j \epsilon_{t-j} + \epsilon_t .
  (\#eq:MSARIMAExpanded)
\end{equation}
Here $K$ is the order of the highest polynomial, calculated as $K=\max\left(\sum_{j=0}^n (P_j + D_j)m_j, \sum_{j=0}^n Q_j m_j\right)$. If, for example, the MA order is higher than the sum of ARI orders, then polynomials $\eta_i=0$ for $i>\sum_{j=0}^n (P_j + D_j)m_j$. The same property holds for the opposite situation of the sum of ARI orders being higher than the MA orders. Based on this we could define states for each of the previous elements:
\begin{equation}
  v_{i,t-i} = \eta_i y_{t-i} + \theta_i \epsilon_{t-i},
  (\#eq:MSARIMAState)
\end{equation}
leading to the following model based on \@ref(eq:MSARIMAState) and \@ref(eq:MSARIMAExpanded):
\begin{equation}
  y_t = \sum_{j=1}^K v_{j,t-j} + \epsilon_t .
  (\#eq:MSARIMAMeasurement01)
\end{equation}
This can be considered as a measurement equation of the state space ARIMA. Now if we consider the previous values of $y_t$ based on \@ref(eq:MSARIMAMeasurement01), for $y_{t-i}$, it will be equal to:
\begin{equation}
  y_{t-i} = \sum_{j=1}^K v_{j,t-j-i} + \epsilon_{t-i} .
  (\#eq:MSARIMAMeasurement02)
\end{equation}
The value \@ref(eq:MSARIMAMeasurement02) can be inserted into \@ref(eq:MSARIMAState), in order to get the transition equation:
\begin{equation}
  v_{i,t-i} = \eta_i \sum_{j=1}^K v_{j,t-j-i} + (\eta_i + \theta_i) \epsilon_{t-i}.
  (\#eq:MSARIMATransition)
\end{equation}
This leads to the SSOE state space model based on \@ref(eq:MSARIMAMeasurement02) and \@ref(eq:MSARIMATransition):
\begin{equation}
  \begin{aligned}
    &{y}_{t} = \sum_{j=1}^K v_{j,t-j} + \epsilon_t \\
    &v_{i,t} = \eta_i \sum_{j=1}^K v_{j,t-j} + (\eta_i + \theta_i) \epsilon_{t} \text{ for each } i=\{1, 2, \dots, K \}
  \end{aligned},
  (\#eq:ADAMARIMAExpanded)
\end{equation}
which can be formulated in the conventional form:
\begin{equation*}
  \begin{aligned}
    &{y}_{t} = \mathbf{w}' \mathbf{v}_{t-\mathbf{l}} + \epsilon_t \\
    &\mathbf{v}_{t} = \mathbf{F} \mathbf{v}_{t-\mathbf{l}} + \mathbf{g} \epsilon_t
  \end{aligned},
\end{equation*}
with the following values for matrices:
\begin{equation}
  \begin{aligned}
    \mathbf{F} = \begin{pmatrix} \eta_1 & \eta_1 & \dots & \eta_1 \\ \eta_2 & \eta_2 & \dots & \eta_1 \\ \vdots & \vdots & \ddots & \vdots \\ \eta_K & \eta_K & \dots & \eta_K \end{pmatrix}, & \mathbf{w} = \begin{pmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{pmatrix}, \\
    \mathbf{g} = \begin{pmatrix} \eta_1 + \theta_1 \\ \eta_2 + \theta_2 \\ \vdots \\ \eta_K + \theta_K \end{pmatrix}, & \mathbf{v}_{t} = \begin{pmatrix} v_{1,t} \\ v_{2,t} \\ \vdots \\ v_{K,t} \end{pmatrix}, & \mathbf{l} = \begin{pmatrix} 1 \\ 2 \\ \vdots \\ K \end{pmatrix}
  \end{aligned}.
  (\#eq:ADAMARIMAMatrices)
\end{equation}
States in this model do not have any specific meaning, they just represent a combination of actual values and error terms, some portion of ARIMA model. Furthermore, there are zero states in this model, corresponding to zero polynomials of ARI and MA. These can be dropped to make the model even more compact.

### An example
In order to better understand what the state space model \@ref(eq:ADAMARIMAExpanded) implies, we consider an example of SARIMA(1,1,2)(0,1,0)$_4$:
\begin{equation*}
    {y}_{t} (1- \phi_1 B)(1-B)(1-B^4) = \epsilon_t (1 + \theta_1 B + \theta_2 B^2),
\end{equation*}
which can be rewritten in the expanded form:
\begin{equation*}
    {y}_{t} (1-\phi_1 B - B + \phi_1 B^2 - B^4 +\phi_1 B^5 + B^5 - \phi_1 B^6) = \epsilon_t (1 + \theta_1 B + \theta_2 B^2),
\end{equation*}
or after moving the previous values to the right hand side:
\begin{equation*}
    {y}_{t} = (1+\phi_1) {y}_{t-1} - \phi_1 {y}_{t-2} + {y}_{t-4} - (1+\phi_1) {y}_{t-5} + \phi_1 {y}_{t-6} + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \epsilon_t .
\end{equation*}
The polynomials in this case can be written as:
\begin{equation*}
    \begin{aligned}
    & \eta_1 = 1+\phi_1 \\
    & \eta_2 = -\phi_1 \\
    & \eta_3 = 0 \\
    & \eta_4 = 1 \\
    & \eta_5 = - (1+\phi_1) \\
    & \eta_6 = \phi_1
    \end{aligned} ,
\end{equation*}
leading to 6 states, one of which can be dropped (the third one, for which both $\eta_3=0$ and $\theta_3=0$). The state space ARIMA can then be written as:
\begin{equation*}
    \begin{aligned}
    &{y}_{t} = \sum_{j=1}^6 v_{j,t-j} + \epsilon_t \\
    & v_{1,t} = (1+\phi_1) \sum_{j=1}^6 v_{j,t-j} + (1+\phi_1+\theta_1) \epsilon_t \\
    & v_{2,t} = -\phi_1 \sum_{j=1}^6 v_{j,t-j} + (-\phi_1+\theta_2) \epsilon_t \\
    & v_{4,t} = \sum_{j=1}^6 v_{j,t-j} + \epsilon_t \\
    & v_{5,t} = -(1+\phi_1) \sum_{j=1}^6 v_{j,t-j} -(1+\phi_1) \epsilon_t \\
    & v_{6,t} = \phi_1 \sum_{j=1}^6 v_{j,t-j} + \phi_1 \epsilon_t
    \end{aligned} .
\end{equation*}
This model looks more complicated than the original ARIMA in the conventional form, but it bring the model to the same ground as [ETS in ADAM](#ADAMETSIntroduction), making them directly comparable via information criteria and allowing to easily combine the two models, not to mention compare ARIMA of any order with another ARIMA (e.g. with different orders of differencing).

### State space ARIMA with constant
If we want to add the [constant](#ARMAConstant) to the model, we need to modify the equation \@ref(eq:MSARIMAExpanded):
\begin{equation}
  y_t = \sum_{j=1}^K \eta_j y_{t-j} + \sum_{j=1}^K \theta_j \epsilon_{t-j} + a_0 + \epsilon_t .
  (\#eq:MSARIMAExpandedConstant)
\end{equation}
This then leads to the appearance of the new state:
\begin{equation}
  v_{K+1,t} = a_0 ,
  (\#eq:MSARIMAStateConstant)
\end{equation}
which leads to the modified measurement equation:
\begin{equation}
  y_t = \sum_{j=1}^{K+1} v_{j,t-j} + \epsilon_t ,
  (\#eq:MSARIMAMeasurementConstant)
\end{equation}
and the modified transition states:
\begin{equation}
  \begin{aligned}
    & v_{i,t} = \eta_i \sum_{j=1}^{K+1} v_{j,t-j} + (\eta_i + \theta_i) \epsilon_{t} , \text{ for } i=\{1, 2, \dots, K\} \\
    & v_{K+1, t} = v_{K+1, t-1} .
  \end{aligned}
  (\#eq:MSARIMATransitionConstant)
\end{equation}
Note that the constant term introduced in this model has different meaning, depending on the differences of the model. For example, if all $D_j=0$, then it acts as an intercept, while for the $d=1$, it will act as a drift.

### Initialising ADAM ARIMA
Each state $v_{i,t}$ needs to be initialised with $i$ values (e.g. 1 for the first state, 2 for the second etc). This leads in general to more initial values for states than the SSARIMA from @Svetunkov2019: $\frac{K(K+1)}{2}$ instead of $K$. However, this formulation has a more compact transition matrix, leading to computational improvements in terms of applying the model to the data with large $K$ (e.g. multiple seasonalities). Besides, we can reduce the number of initial seeds to estimate either by using a different initialisation procedure (e.g. backcasting) or estimating directly $y_t$ and $\epsilon_t$ for $t=\{-K+1, -K+2, \dots, 0\}$ to obtain the initials for each state via the formula \@ref(eq:MSARIMATransition). In order to reduce the number of estimated parameters to $K$, we can take the conditional expectations for the states, in which case we will have:
\begin{equation*}
  \mathrm{E}(v_{i,t} | t) = \eta_i y_{t} \text{ for } t=\{-K+1, -K+2, \dots, 0\},
\end{equation*}
and then use these expectations for the initialisation of ARIMA. A the same time, we can express the actual value in terms of the state and error from \@ref(eq:MSARIMAState) for the last state $K$:
\begin{equation}
  y_{t} = \frac{v_{K,t} - \theta_K \epsilon_{t}}{\eta_K}.
  (\#eq:MSARIMAInitialisation01)
\end{equation}
We select the last state $K$ because it has the highest number of initials to estimate among all states. We can then insert the value \@ref(eq:MSARIMAInitialisation01) in each formula for each state for $i=\{1, 2, \dots, K-1\}$ and take their expectations:
\begin{equation}
  \mathrm{E}(v_{i,t}|t) = \frac{\eta_i}{\eta_K} \mathrm{E}(v_{K,t}|t)  \text{ for } t=\{-i+1, -i+2, \dots, 0\}.
  (\#eq:MSARIMAInitialisation02)
\end{equation}
So the process then comes to estimating the initials states of $v_{K,t}$ for $t=\{-K+1, -K+2, \dots, 0\}$ and then propagating them to the other states. However, this strategy will not work for models, when ARI polynomials are shorter than MA ones, i.e. $\sum_{j=0}^n (P_j + D_j)m_j < \sum_{j=0}^n Q_j m_j $. But using the same principle of initialisation via the conditional expectation, we can set the initial MA states to zero and estimate only ARI states.

<!-- ## Multiplicative error ARIMA -->

<!-- ## Stability and Stationarity conditions -->

<!-- ## Forecasting with ADAM ARIMA -->




<!-- First we assume that there exists non-zero $\eta_K$, implying that $\sum_{j=0}^n (P_j + D_j)m_j \geq \sum_{j=0}^n Q_j m_j $. This does not necessarily hold, because there can be models like ARIMA(0,1,2), where the ARI(p,d) polynomial only contains the value for $i=1$, while the MA polynomial contains both $\theta_1$ and $\theta_2$. But we will discuss later, what can be done in this situation. So, for now we assume that there exists non-zero $\eta_K$. -->

<!-- What we need to do in this case is to calculate the initial values of the final state using \@ref(eq:MSARIMAState): -->
<!-- \begin{equation*} -->
<!--   v_{K,t} = \eta_K y_{t} + \theta_K \epsilon_{t} \text{ for } t=\{-K+1, -K+2, \dots, 0\}, -->
<!-- \end{equation*} -->
<!-- whch can be done if we estimate the actual values $y_t$ and the error term $\epsilon_t$ for $t=\{-K+1, -K+2, \dots, 0\}$ either via the numerical optimisation or via backcasting. At the same time, we can express the actual value in terms of the state and error: -->
<!-- \begin{equation} -->
<!--   y_{t} = \frac{v_{K,t} - \theta_K \epsilon_{t}}{\eta_K}. -->
<!--   (\#eq:MSARIMAInitialisation01) -->
<!-- \end{equation} -->
<!-- We select the last state $K$ because it has the highest number of initials to estimate among all states. We can then insert the value \@ref(eq:MSARIMAInitialisation01) into each formula for each state for $i=\{1, 2, \dots, K-1\}$: -->
<!-- \begin{equation} -->
<!--   v_{i,t} = \frac{v_{K,t} - \theta_K \epsilon_{t}}{\eta_K} + \theta_i \epsilon_{t} = \frac{\eta_i}{\eta_K} v_{K,t} + \left( \theta_i - \frac{\theta_K}{\eta_K} \right) \epsilon_{t} . -->
<!--   (\#eq:MSARIMAInitialisation02) -->
<!-- \end{equation} -->
<!-- We then take the expectation of the state \@ref(eq:MSARIMAInitialisation02) to get the formula for the initialisation of states: -->
<!-- \begin{equation} -->
<!--   v_{i,t} = \frac{\eta_i}{\eta_K} v_{K,t} . -->
<!--   (\#eq:MSARIMAInitialisation03) -->
<!-- \end{equation} -->
<!-- The indices can be then moved back in time to get the initials before the $t=1$. -->

<!-- ### MA initialisation -->
<!-- An alternative way to initialise the model is to use MA polynomials, which becomes useful for the cases, when $\sum_{j=0}^n (P_j + D_j)m_j < \sum_{j=0}^n Q_j m_j$, meaning that $\eta_K = 0$. The logic in this case is similar. The final state $v_{K,t}$ will be equal in this situation to: -->
<!-- \begin{equation*} -->
<!--   v_{K,t} = \theta_K \epsilon_{t}, -->
<!-- \end{equation*} -->
<!-- meaning that -->
<!-- \begin{equation} -->
<!--   \epsilon_{t} = \frac{v_{K,t}}{\theta_K}, -->
<!--   (\#eq:MSARIMAInitialisation04) -->
<!-- \end{equation} -->
<!-- which can be inserted in the formula for the states \@ref(eq:MSARIMAState): -->
<!-- \begin{equation} -->
<!--   v_{i,t} = \eta_i y_{t} + \frac{\theta_i}{\theta_K}v_{K,t}, -->
<!--   (\#eq:MSARIMAStateTheta) -->
<!-- \end{equation} -->

<!-- ## ETS + ARIMA -->

<!-- ## Examples in R -->
